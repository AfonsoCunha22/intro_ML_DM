{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network (Project 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression goal\n",
    "- We want to predict the target value streams based on certain input columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Considerations\n",
    "\n",
    "- How many hidden layers?\n",
    "- How many hidden units?\n",
    "- What kind of layers?\n",
    "- What performance is adequate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "\n",
    "- Read data\n",
    "- One-hot-encoding\n",
    "- Remove outliers\n",
    "- Keep columns of interest\n",
    "- Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import precision_score, mean_squared_error, recall_score, f1_score, roc_auc_score, roc_curve, mean_squared_error\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.stats import ttest_rel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"dataset/dataset-spotify-2023.csv\"\n",
    "data = pd.read_csv(dataset_path, encoding=\"latin-1\")\n",
    "columns = [\"danceability_%\", \"valence_%\", \"energy_%\", \"acousticness_%\", \"instrumentalness_%\",\n",
    "\"liveness_%\", \"speechiness_%\"]\n",
    "data = data.rename(columns={column: column.replace(\"_%\", \"\") for column in columns})\n",
    "key_None_count = data[\"key\"].isna().sum()\n",
    "in_shazam_charts_None_count = data[\"in_shazam_charts\"].isna().sum()\n",
    "\n",
    "print(\"`key` None count: \", key_None_count)\n",
    "print(\"`in_shazam_charts` None count: \", in_shazam_charts_None_count)\n",
    "\n",
    "# Replace NaN values with Unspecified, it may be useful later on\n",
    "data = data.replace(np.nan, \"Unavailable\")\n",
    "\n",
    "# Data is malformed, need to remove comma `,`\n",
    "data[\"in_deezer_playlists\"] = data[\"in_deezer_playlists\"].replace(\",\", \"\", regex=True)\n",
    "data[\"in_shazam_charts\"] = data[\"in_deezer_playlists\"].replace(\",\", \"\", regex=True)\n",
    "\n",
    "# Convert columns to int64\n",
    "# streams, in_deezer_playlists, in_shazam_charts\n",
    "data[\"in_deezer_playlists\"] = data[\"in_deezer_playlists\"].astype(int)\n",
    "data[\"in_shazam_charts\"] = data[\"in_shazam_charts\"].astype(int)\n",
    "\n",
    "# Streams overflowed with int, so use np.int64 to fit the whole numbers\n",
    "data[\"streams\"] = data[\"streams\"].astype(np.int64)\n",
    "\n",
    "# Wee see that `streams` is very large compared to to other data, next larger is `in_spotify_playlists`\n",
    "# Add extra column with log value of streams\n",
    "# No need for that, data is normalized later anyway\n",
    "# data[\"streams_log\"] = np.log2(data[\"streams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding values\n",
    "data = pd.get_dummies(data, columns=[\"key\", \"mode\"], prefix=[\"key\", \"mode\"])\n",
    "data = data.applymap(lambda x: int(x) if isinstance(x, bool) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numeric columns\n",
    "data_numeric = data.select_dtypes(exclude=\"object\")\n",
    "data_numeric.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not working in our favor\n",
    "# I think we can test that with the new dataset\n",
    "# z_scores = np.abs(stats.zscore(data_numeric))\n",
    "# data_numeric = data_numeric[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_numeric)\n",
    "data_numeric = pd.DataFrame(data_normalized, columns=data_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected columns:\n",
    "# Columns that express musicality and composition\n",
    "composition_features = [\"bpm\", \n",
    "                        \"mode_Major\",\n",
    "                        \"mode_Minor\",\n",
    "                        \"key_A\",\n",
    "                        \"key_A#\",\n",
    "                        \"key_B\",\n",
    "                        \"key_C#\",\n",
    "                        \"key_D\",\n",
    "                        \"key_D#\",\n",
    "                        \"key_E\",\n",
    "                        \"key_F\",\n",
    "                        \"key_F#\",\n",
    "                        \"key_G\",\n",
    "                        \"key_G#\", \n",
    "                        \"danceability\",\n",
    "                        \"valence\",\n",
    "                        \"energy\",\n",
    "                        \"acousticness\", \n",
    "                        \"instrumentalness\",\n",
    "                        \"liveness\",\n",
    "                        \"speechiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test columns\n",
    "minimal_features = [\"in_spotify_playlists\",\n",
    "                    \"in_apple_charts\",\n",
    "                    \"in_shazam_charts\",\n",
    "                    \"in_apple_playlists\",\n",
    "                    \"bpm\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Target value is `streams`\n",
    "X = torch.tensor(data_numeric[minimal_features].values, dtype=torch.float32)\n",
    "Y = torch.tensor(data_numeric[\"streams\"].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a TensorDataset with X and Y\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "# Let's assume we'll use 80% of the data for training and 20% for validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "# Use random_split to create the training and validation subsets\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# Create DataLoaders for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1] \n",
    "output_size = Y.shape[1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline model\n",
    "baseline_model = DummyClassifier(strategy='most_frequent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple ANN\n",
    "# In total, the network is composed of two layers: one hidden layer (self.fc1 + ReLU) and one output layer (self.fc2).\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_unit_size=32):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_unit_size)\n",
    "        self.fc2 = nn.Linear(hidden_unit_size, output_size)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Linear Regression\n",
    "class LinearRegression(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lambda: 0.01, Average Cross-Validation Mean Squared Error: 0.30265880666355516\n",
      "Test Mean Squared Error: 0.25632091966107\n",
      "\n",
      "Composition Features Model\n",
      "Coefficients: [[ 0.54790255  0.00995077 -0.03502727  0.39528954 -0.00516066]]\n",
      "Intercept: [0.00956632]\n",
      "\n",
      "\n",
      "Lambda: 0.1, Average Cross-Validation Mean Squared Error: 0.3026530317761874\n",
      "Test Mean Squared Error: 0.25630438182503934\n",
      "\n",
      "Composition Features Model\n",
      "Coefficients: [[ 0.54763773  0.00997351 -0.03482801  0.39533179 -0.005162  ]]\n",
      "Intercept: [0.00956836]\n",
      "\n",
      "\n",
      "Lambda: 1, Average Cross-Validation Mean Squared Error: 0.30259842261935876\n",
      "Test Mean Squared Error: 0.25614267647772454\n",
      "\n",
      "Composition Features Model\n",
      "Coefficients: [[ 0.54501871  0.01020079 -0.03285658  0.39574296 -0.00517502]]\n",
      "Intercept: [0.00958857]\n",
      "\n",
      "\n",
      "Lambda: 10, Average Cross-Validation Mean Squared Error: 0.302313224251178\n",
      "Test Mean Squared Error: 0.25484181564310737\n",
      "\n",
      "Composition Features Model\n",
      "Coefficients: [[ 0.52143561  0.01245544 -0.01504408  0.39884991 -0.00527183]]\n",
      "Intercept: [0.00977427]\n",
      "\n",
      "\n",
      "Lambda: 100000000, Average Cross-Validation Mean Squared Error: 1.0595351227590883\n",
      "Test Mean Squared Error: 0.7712969341394087\n",
      "\n",
      "Composition Features Model\n",
      "Coefficients: [[ 6.37973453e-06  2.66487922e-06  4.78431333e-06  6.23808950e-06\n",
      "  -2.72991544e-08]]\n",
      "Intercept: [0.01871078]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Regularized Linear Regression\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define a range of values for lambda (regularization parameter)\n",
    "lambda_values = [0.01, 0.1, 1, 10, 10**8]\n",
    "\n",
    "# Perform cross-validation for each lambda value\n",
    "for lambda_val in lambda_values:\n",
    "    # Creating a Ridge regression model with regularization parameter lambda\n",
    "    model = Ridge(alpha=lambda_val)\n",
    "\n",
    "    # Perform 10-fold cross-validation and compute mean squared error for each fold\n",
    "    cv_scores = cross_val_score(\n",
    "        model, X_train, y_train, cv=10, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Calculate the average cross-validation score\n",
    "    avg_mse = np.mean(-cv_scores)\n",
    "\n",
    "    print(f\"Lambda: {lambda_val}, Average Cross-Validation Mean Squared Error: {avg_mse}\")\n",
    "\n",
    "    # Train the model on the entire training set\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Making predictions on the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # Evaluating the model on the test set\n",
    "    test_mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"Test Mean Squared Error: {test_mse}\\n\")\n",
    "    print(\"Composition Features Model\")\n",
    "    print(\"Coefficients:\", model.coef_)\n",
    "    print(\"Intercept:\", model.intercept_)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, train_loader, valid_loader, patience=25):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        total_valid_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        epoch_valid_loss = total_valid_loss / len(valid_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_valid_loss:.4f}\")\n",
    "\n",
    "\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate_model(model, test_loader, criterion):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    total_test_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_test_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader.dataset)\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two-Level Cross Validation parameters\n",
    "K1, K2 = 10, 10\n",
    "hidden_units_options = [1, 8, 16, 32, 64]\n",
    "num_epochs = 500\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "inner_cv = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "for train_index, test_index in outer_cv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    best_model_state = None\n",
    "    best_error = float('inf')\n",
    "    best_h = None\n",
    "    \n",
    "    for h in hidden_units_options:\n",
    "        inner_errors = []\n",
    "        for inner_train_index, inner_val_index in inner_cv.split(X_train):\n",
    "            # Split and convert data, create DataLoaders...\n",
    "\n",
    "            # Initialize model\n",
    "            simpleANN = SimpleANN(input_size=X_train.shape[1], output_size=output_size, hidden_unit_size=h)\n",
    "            optimizer = torch.optim.Adam(simpleANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "            # Train the model\n",
    "            train(simpleANN, criterion, optimizer, num_epochs, train_loader, valid_loader)\n",
    "\n",
    "            # Evaluate the model on the validation set\n",
    "            validation_error = evaluate_model(simpleANN, valid_loader, criterion)\n",
    "            inner_errors.append(validation_error)\n",
    "\n",
    "        avg_inner_error = np.mean(inner_errors)\n",
    "\n",
    "        if avg_inner_error < best_error:\n",
    "            best_error = avg_inner_error\n",
    "            best_model_state = simpleANN.state_dict()\n",
    "            best_h = h\n",
    "\n",
    "# After outer loop, load best_model_state into a new model for final evaluation\n",
    "best_model = SimpleANN(input_size=X.shape[1], output_size=output_size, hidden_unit_size=best_h)\n",
    "best_model.load_state_dict(best_model_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = 64 model \n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "K1, K2 = 10, 10\n",
    "hidden_units_options = [1, 8, 16, 32, 64]\n",
    "lambda_options = [0.01, 0.1, 1, 10, 100]  # Example lambda values\n",
    "num_epochs = 1000\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 32  # Example batch size\n",
    "\n",
    "# Prepare K-Fold cross validation\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Outer Loop\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X)):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
    "\n",
    "    best_h = None\n",
    "    best_lambda = None\n",
    "    best_error = float('inf')\n",
    "\n",
    "    # Inner Loop for Model Selection\n",
    "    for h in hidden_units_options:\n",
    "        for lambda_val in lambda_options:\n",
    "            inner_cv = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "            inner_errors = []\n",
    "\n",
    "            for inner_train_index, inner_val_index in inner_cv.split(X_train):\n",
    "                # Data preparation for inner loop\n",
    "                X_inner_train, X_inner_val = X_train[inner_train_index], X_train[inner_val_index]\n",
    "                Y_inner_train, Y_inner_val = Y_train[inner_train_index], Y_train[inner_val_index]\n",
    "\n",
    "                # Convert to tensors and create DataLoaders\n",
    "                train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_inner_train, dtype=torch.float32),\n",
    "                                                               torch.tensor(Y_inner_train, dtype=torch.float32).view(-1, 1))\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "                valid_dataset = torch.utils.data.TensorDataset(torch.tensor(X_inner_val, dtype=torch.float32),\n",
    "                                                               torch.tensor(Y_inner_val, dtype=torch.float32).view(-1, 1))\n",
    "                valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                # Model initialization\n",
    "                model = SimpleANN(input_size=X_train.shape[1], output_size=output_size, hidden_unit_size=h)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=lambda_val)\n",
    "\n",
    "                # Train the model\n",
    "                train(model, criterion, optimizer, num_epochs, train_loader, valid_loader)\n",
    "\n",
    "                # Evaluate the model\n",
    "                validation_error = evaluate_model(model, valid_loader, criterion)\n",
    "                inner_errors.append(validation_error)\n",
    "\n",
    "            avg_inner_error = np.mean(inner_errors)\n",
    "\n",
    "            if avg_inner_error < best_error:\n",
    "                best_h = h\n",
    "                best_lambda = lambda_val\n",
    "                best_error = avg_inner_error\n",
    "\n",
    "    # Train the best model on the entire parameter tuning set\n",
    "    # Convert to tensors and create DataLoaders for the full training set\n",
    "    full_train_dataset = torch.utils.data.TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
    "                                                        torch.tensor(Y_train, dtype=torch.float32).view(-1, 1))\n",
    "    full_train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    best_model = SimpleANN(input_size=X_train.shape[1], output_size=output_size, hidden_unit_size=best_h)\n",
    "    best_optimizer = optim.Adam(best_model.parameters(), lr=0.001, weight_decay=best_lambda)\n",
    "    train(best_model, criterion, best_optimizer, num_epochs, full_train_loader, None)\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    test_dataset = torch.utils.data.TensorDataset(torch.tensor(X_test, dtype=torch.float32),\n",
    "                                                  torch.tensor(Y_test, dtype=torch.float32).view(-1, 1))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_error = evaluate_model(best_model, test_loader, criterion)\n",
    "\n",
    "    # Evaluate the baseline model on the test set \n",
    "    baseline_model.fit(X_train, Y_train)\n",
    "    baseline_error = baseline_model.evaluate(X_test, Y_test)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    results.append({\n",
    "        'Fold': fold_idx + 1,\n",
    "        'Optimal h': best_h,\n",
    "        'Optimal λ': best_lambda,\n",
    "        'Test Error': test_error,\n",
    "        'Baseline Error': baseline_error\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for a nicer table format\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, train_loader):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mirto\\AppData\\Local\\Temp\\ipykernel_3236\\3699358538.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  X_tensor = torch.tensor(X, dtype=torch.float32)\n",
      "C:\\Users\\mirto\\AppData\\Local\\Temp\\ipykernel_3236\\3699358538.py:11: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  Y_tensor = torch.tensor(Y, dtype=torch.float32).view(-1, 1)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DummyClassifier' object has no attribute 'evaluate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\intro_ML_DM\\ann.ipynb Cell 24\u001b[0m line \u001b[0;36m7\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=76'>77</a>\u001b[0m \u001b[39m# Evaluate the baseline model on the test set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m baseline_model\u001b[39m.\u001b[39mfit(X_train\u001b[39m.\u001b[39mnumpy(), Y_train\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m baseline_error \u001b[39m=\u001b[39m baseline_model\u001b[39m.\u001b[39;49mevaluate(X_test\u001b[39m.\u001b[39mnumpy(), Y_test\u001b[39m.\u001b[39mnumpy())\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Store the results for this fold\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m results\u001b[39m.\u001b[39mappend({\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mFold\u001b[39m\u001b[39m'\u001b[39m: fold_idx \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mOptimal h\u001b[39m\u001b[39m'\u001b[39m: best_h,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mBaseline Error\u001b[39m\u001b[39m'\u001b[39m: baseline_error\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/intro_ML_DM/ann.ipynb#X64sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m })\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DummyClassifier' object has no attribute 'evaluate'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters and data preparation\n",
    "K1, K2 = 10, 10\n",
    "hidden_units_options = [1, 8, 16, 32, 64]\n",
    "lambda_options = [0.01, 0.1, 1, 10, 100]\n",
    "num_epochs = 1000\n",
    "criterion = nn.MSELoss()\n",
    "batch_size = 32\n",
    "\n",
    "# Convert numpy arrays to torch tensors\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "Y_tensor = torch.tensor(Y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Prepare K-Fold cross validation\n",
    "outer_cv = KFold(n_splits=K1, shuffle=True, random_state=42)\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "# Outer Loop\n",
    "for fold_idx, (train_index, test_index) in enumerate(outer_cv.split(X_tensor)):\n",
    "    X_train, X_test = X_tensor[train_index], X_tensor[test_index]\n",
    "    Y_train, Y_test = Y_tensor[train_index], Y_tensor[test_index]\n",
    "\n",
    "    # DataLoaders for test set\n",
    "    test_dataset = torch.utils.data.TensorDataset(X_test, Y_test)\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Inner Loop for Model Selection (ANN)\n",
    "    best_h = None\n",
    "    best_lambda = None\n",
    "    best_error = float('inf')\n",
    "    for h in hidden_units_options:\n",
    "        for lambda_val in lambda_options:\n",
    "            inner_cv = KFold(n_splits=K2, shuffle=True, random_state=42)\n",
    "            inner_errors = []\n",
    "\n",
    "            for inner_train_index, inner_val_index in inner_cv.split(X_train):\n",
    "                X_inner_train, X_inner_val = X_train[inner_train_index], X_train[inner_val_index]\n",
    "                Y_inner_train, Y_inner_val = Y_train[inner_train_index], Y_train[inner_val_index]\n",
    "\n",
    "                # Create DataLoaders for inner training and validation sets\n",
    "                train_dataset = torch.utils.data.TensorDataset(X_inner_train, Y_inner_train)\n",
    "                train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "                valid_dataset = torch.utils.data.TensorDataset(X_inner_val, Y_inner_val)\n",
    "                valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "                # Model initialization and training\n",
    "                model = SimpleANN(input_size=X_train.shape[1], output_size=output_size, hidden_unit_size=h)\n",
    "                optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=lambda_val)\n",
    "                train(model, criterion, optimizer, num_epochs, train_loader)\n",
    "\n",
    "                # Evaluate the model on the validation set\n",
    "                validation_error = evaluate_model(model, valid_loader, criterion)\n",
    "                inner_errors.append(validation_error)\n",
    "\n",
    "            avg_inner_error = np.mean(inner_errors)\n",
    "\n",
    "            if avg_inner_error < best_error:\n",
    "                best_h = h\n",
    "                best_lambda = lambda_val\n",
    "                best_error = avg_inner_error\n",
    "\n",
    "    # Train the best ANN model on the entire training set\n",
    "    best_ann_model = SimpleANN(input_size=X_train.shape[1], output_size=output_size, hidden_unit_size=best_h)\n",
    "    best_ann_optimizer = optim.Adam(best_ann_model.parameters(), lr=0.001, weight_decay=best_lambda)\n",
    "    full_train_dataset = torch.utils.data.TensorDataset(X_train, Y_train)\n",
    "    full_train_loader = torch.utils.data.DataLoader(full_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    train(best_ann_model, criterion, best_ann_optimizer, num_epochs, full_train_loader)\n",
    "    ann_test_error = evaluate_model(best_ann_model, test_loader, criterion)\n",
    "\n",
    "    # Train the linear regression model on the training set\n",
    "    lin_reg_model = LinearRegression(input_size=X_train.shape[1])\n",
    "    lin_reg_optimizer = optim.Adam(lin_reg_model.parameters(), lr=0.001)\n",
    "    train(lin_reg_model, criterion, lin_reg_optimizer, num_epochs, full_train_loader)\n",
    "    lin_reg_test_error = evaluate_model(lin_reg_model, test_loader, criterion)\n",
    "\n",
    "    # Evaluate the baseline model on the test set\n",
    "    baseline_model = DummyClassifier(strategy=\"most_frequent\")\n",
    "    baseline_model.fit(X_train.numpy(), Y_train.numpy().ravel())\n",
    "    baseline_predictions = baseline_model.predict(X_test.numpy())\n",
    "    baseline_error = mean_squared_error(Y_test.numpy(), baseline_predictions)\n",
    "\n",
    "    # Store the results for this fold\n",
    "    results.append({\n",
    "        'Fold': fold_idx + 1,\n",
    "        'Optimal h': best_h,\n",
    "        'Optimal λ': best_lambda,\n",
    "        'ANN Test Error': ann_test_error,\n",
    "        'Linear Regression Test Error': lin_reg_test_error,\n",
    "        'Baseline Error': baseline_error\n",
    "    })\n",
    "\n",
    "# Convert results to a DataFrame for a nicer table format\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Extract the test errors for each model\n",
    "ann_errors = [result['ANN Test Error'] for result in results]\n",
    "lin_reg_errors = [result['Linear Regression Test Error'] for result in results]\n",
    "baseline_errors = [result['Baseline Error'] for result in results]\n",
    "\n",
    "# Perform paired t-tests\n",
    "t_statistic, p_value_ann_vs_lin = ttest_rel(ann_errors, lin_reg_errors)\n",
    "t_statistic, p_value_ann_vs_base = ttest_rel(ann_errors, baseline_errors)\n",
    "t_statistic, p_value_lin_vs_base = ttest_rel(lin_reg_errors, baseline_errors)\n",
    "\n",
    "# Print p-values\n",
    "print(\"P-value for ANN vs. Linear Regression:\", p_value_ann_vs_lin)\n",
    "print(\"P-value for ANN vs. Baseline:\", p_value_ann_vs_base)\n",
    "print(\"P-value for Linear Regression vs. Baseline:\", p_value_lin_vs_base)\n",
    "\n",
    "# Display the results\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a more complex ANN with more layers (no dropouts)\n",
    "\n",
    "class ComplexANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[32, 64]):\n",
    "        super(ComplexANN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for k in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            nn.init.xavier_uniform_(self.hidden_layers[k].weight)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the finalized, parameter tuned ANN\n",
    "# We add dropout layers\n",
    "\n",
    "class FinalANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[32, 64], dropout_prob=0.5):\n",
    "        super(FinalANN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for k in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            nn.init.xavier_uniform_(self.hidden_layers[k].weight)\n",
    "            self.hidden_layers.append(nn.Dropout(p=dropout_prob))\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "\n",
    "        for layer in self.hidden_layers:\n",
    "            if isinstance(layer, nn.Dropout):\n",
    "                x = layer(x)\n",
    "            else:\n",
    "                x = F.relu(layer(x))\n",
    "\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the test errors for each model\n",
    "ann_errors = [result['ANN Test Error'] for result in results]\n",
    "lin_reg_errors = [result['Linear Regression Test Error'] for result in results]\n",
    "baseline_errors = [result['Baseline Error'] for result in results]\n",
    "\n",
    "# Perform paired t-tests\n",
    "t_statistic, p_value_ann_vs_lin = ttest_rel(ann_errors, lin_reg_errors)\n",
    "t_statistic, p_value_ann_vs_base = ttest_rel(ann_errors, baseline_errors)\n",
    "t_statistic, p_value_lin_vs_base = ttest_rel(lin_reg_errors, baseline_errors)\n",
    "\n",
    "# Print p-values\n",
    "print(\"P-value for ANN vs. Linear Regression:\", p_value_ann_vs_lin)\n",
    "print(\"P-value for ANN vs. Baseline:\", p_value_ann_vs_base)\n",
    "print(\"P-value for Linear Regression vs. Baseline:\", p_value_lin_vs_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 1000\n",
    "# For regression, it is recommended to use MSELoss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model SimpleANN\n",
    "simpleANN = SimpleANN(input_size, output_size)\n",
    "# Add weight decay to the optimizer\n",
    "optimizer = torch.optim.Adam(simpleANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "train(simpleANN, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model ComplexANN\n",
    "complexANN = ComplexANN(input_size, output_size, hidden_sizes=[64, 64])\n",
    "optimizer = torch.optim.Adam(complexANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "train(complexANN, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model FinalANN\n",
    "finalANN = FinalANN(input_size, output_size, hidden_sizes=[64, 64])\n",
    "optimizer = torch.optim.Adam(finalANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "train(finalANN, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification goal\n",
    "\n",
    "We use the composition features. The columns: \"bpm\", \"mode_Major\", \"mode_Minor\", \"key_A\", \"key_A#\", \"key_B\", \"key_C#\", \"key_D\", \"key_D#\", \"key_E\", \"key_F\", \"key_F#\", \"key_G\", \"key_G#\", \"danceability\", \"valence\", \"energy\", \"acousticness\", \"instrumentalness\", \"liveness\", \"speechiness\" \n",
    "\n",
    "The classification problem described is a **binary** classification problem. \n",
    "Specifically, the goal is to categorize songs into one of two classes: \"hit\" or \"not hit\" based on their stream counts. \n",
    "The definition of a \"hit\" is based on a percentile analysis, where songs falling within the top 10% of stream counts are considered hits, \n",
    "and the remaining 90% are considered not hits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hit threshold and prepare data\n",
    "hit_threshold = data_numeric[\"streams\"].quantile(0.9)\n",
    "\n",
    "data_numeric[\"hit\"] = (data_numeric[\"streams\"] >= hit_threshold).astype(int)\n",
    "\n",
    "bl_X = data_numeric[composition_features].values\n",
    "bl_y = data_numeric[\"hit\"].values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bl_X = scaler.fit_transform(bl_X)\n",
    "\n",
    "bl_X_tensor = torch.tensor(bl_X, dtype=torch.float32)\n",
    "bl_y_tensor = torch.tensor(bl_y, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "bl_X_train, bl_X_test, bl_y_train, bl_y_test = train_test_split(\n",
    "    bl_X_tensor, bl_y_tensor, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = TensorDataset(bl_X_train, bl_y_train)\n",
    "valid_dataset = TensorDataset(bl_X_test, bl_y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification NN for \"hits\" task\n",
    "class SimpleClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))  \n",
    "        x = F.relu(self.fc2(x)) \n",
    "        x = torch.sigmoid(self.fc3(x)) \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to make a more complex classification model to see if there is any improvement in the performance\n",
    "\n",
    "class ComplexClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(ComplexClassificationNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.fc4 = nn.Linear(64, 32)\n",
    "        self.fc5 = nn.Linear(32, 1)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "        nn.init.xavier_uniform_(self.fc3.weight)\n",
    "        nn.init.xavier_uniform_(self.fc4.weight)\n",
    "        nn.init.xavier_uniform_(self.fc5.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = F.relu(self.fc4(x))\n",
    "        x = torch.sigmoid(self.fc5(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, train_loader, valid_loader, patience=25):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    best_valid_loss = np.inf\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.view(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        total_valid_loss = 0\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets.view(-1, 1))\n",
    "                total_valid_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        epoch_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        epoch_valid_loss = total_valid_loss / len(valid_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_valid_loss:.4f}\")\n",
    "\n",
    "        if epoch_valid_loss < best_valid_loss:\n",
    "            best_valid_loss = epoch_valid_loss\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered. No improvement for {patience} epochs.\")\n",
    "            break\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(1, len(valid_losses) + 1), valid_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SimpleClassificationNN test\n",
    "input_size = bl_X_train.shape[1]\n",
    "# This is the appropriate loss for binary classification\n",
    "criterion = nn.BCELoss()\n",
    "simple_classification_model = SimpleClassificationNN(input_size)\n",
    "optimizer = torch.optim.Adam(simple_classification_model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "train(simple_classification_model, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ComplexClassificationNN test\n",
    "criterion = nn.BCELoss()\n",
    "complex_classification_model = ComplexClassificationNN(input_size)\n",
    "optimizer = torch.optim.Adam(complex_classification_model.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "train(complex_classification_model, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_and_plot_metrics(model, data_loader):\n",
    "    model.eval()\n",
    "    all_targets = []\n",
    "    all_prob_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            logits = model(inputs)\n",
    "            probabilities = torch.sigmoid(logits)\n",
    "            all_prob_scores.extend(probabilities.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "\n",
    "    all_prob_scores = np.array(all_prob_scores).flatten()\n",
    "    all_targets = np.array(all_targets).flatten()\n",
    "    pred_labels = (all_prob_scores >= 0.5).astype(int)\n",
    "\n",
    "\n",
    "    precision = precision_score(all_targets, pred_labels, zero_division=0)\n",
    "    recall = recall_score(all_targets, pred_labels)\n",
    "    f1 = f1_score(all_targets, pred_labels)\n",
    "    roc_auc = roc_auc_score(all_targets, all_prob_scores)\n",
    "\n",
    "    print(f'Precision: {precision:.4f}')\n",
    "    print(f'Recall: {recall:.4f}')\n",
    "    print(f'F1 Score: {f1:.4f}')\n",
    "    print(f'ROC AUC: {roc_auc:.4f}')\n",
    "\n",
    "\n",
    "    fpr, tpr, _ = roc_curve(all_targets, all_prob_scores)\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.4f})')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    return precision, recall, f1, roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, f1, roc_auc = evaluate_and_plot_metrics(simple_classification_model, valid_loader)\n",
    "precision, recall, f1, roc_auc = evaluate_and_plot_metrics(complex_classification_model, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# McNemera's test (Box 11.3.2 ???? It was not that)\n",
    "def mcnemar_confidence_interval(b, c, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Calculate the confidence interval for the difference in paired proportions (b-c).\n",
    "    Uses the normal approximation for the distribution of the test statistic under the null hypothesis.\n",
    "    \n",
    "    :param b: The number of instances classifier 1 is correct and classifier 2 is not.\n",
    "    :param c: The number of instances classifier 2 is correct and classifier 1 is not.\n",
    "    :param alpha: Significance level for the confidence interval.\n",
    "    :return: The confidence interval as a tuple (lower_bound, upper_bound).\n",
    "    \"\"\"\n",
    "    z_score = stats.norm.ppf(1 - alpha/2)\n",
    "    standard_error = np.sqrt(b + c)\n",
    "    point_estimate = b - c\n",
    "    margin_of_error = z_score * standard_error\n",
    "    \n",
    "    lower_bound = point_estimate - margin_of_error\n",
    "    upper_bound = point_estimate + margin_of_error\n",
    "    \n",
    "    return lower_bound, upper_bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(model, data_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_predictions = []\n",
    "    \n",
    "    with torch.no_grad():  # No need to track gradients for prediction\n",
    "        for inputs, _ in data_loader:\n",
    "            outputs = model(inputs)\n",
    "            predicted_classes = (outputs > 0.5).type(torch.float)  # Apply a threshold to get binary class predictions\n",
    "            all_predictions.extend(predicted_classes.cpu().numpy())  # Move predictions to CPU and convert to numpy\n",
    "\n",
    "    return np.array(all_predictions).flatten()  # Flatten to 1D array if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from both models\n",
    "simple_ann_preds = get_predictions(simple_classification_model, valid_loader)\n",
    "complex_ann_preds = get_predictions(complex_classification_model, valid_loader)\n",
    "\n",
    "# Assuming bl_y_test is a numpy array containing the true binary class labels for the validation set\n",
    "true_labels = bl_y_test.numpy().flatten()\n",
    "\n",
    "# Calculate the values for b and c from the contingency table\n",
    "b = ((simple_ann_preds == 1) & (complex_ann_preds == 0) & (true_labels == 1)).sum()\n",
    "c = ((simple_ann_preds == 0) & (complex_ann_preds == 1) & (true_labels == 1)).sum()\n",
    "\n",
    "# Perform McNemar's test\n",
    "result = mcnemar([[b, c], [c, b]], exact=False, correction=True)\n",
    "print(f\"Test statistic: {result.statistic}, p-value: {result.pvalue}\")\n",
    "\n",
    "# Calculate the confidence interval if needed\n",
    "ci_lower, ci_upper = mcnemar_confidence_interval(b, c)\n",
    "print(f\"Confidence interval for the difference in paired proportions: ({ci_lower:.2f}, {ci_upper:.2f})\")\n",
    "\n",
    "# Interpret the p-value\n",
    "alpha = 0.05\n",
    "if result.pvalue < alpha:\n",
    "    print(\"Reject the null hypothesis: There is a significant difference between the two classifiers.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference between the two classifiers.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Hypothetical test errors for each model across the K1 folds\n",
    "\n",
    "# Perform paired t-tests\n",
    "t_statistic, p_value_ann_vs_lin = ttest_rel(errors_ann, errors_lin_reg)\n",
    "t_statistic, p_value_ann_vs_base = ttest_rel(errors_ann, errors_baseline)\n",
    "t_statistic, p_value_lin_vs_base = ttest_rel(errors_lin_reg, errors_baseline)\n",
    "\n",
    "# Print p-values\n",
    "print(\"P-value for ANN vs. Linear Regression:\", p_value_ann_vs_lin)\n",
    "print(\"P-value for ANN vs. Baseline:\", p_value_ann_vs_base)\n",
    "print(\"P-value for Linear Regression vs. Baseline:\", p_value_lin_vs_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Testing\n",
    "\n",
    "Metrics for Regression:\n",
    "- R2\n",
    "\n",
    "Metrics for Classification :\n",
    "- Precision and Recall\n",
    "- F1 Score\n",
    "- ROC Curve and AUC\n",
    "\n",
    "Outcomes:\n",
    "- If the model is underperforming, consider increasing the number of **layers** or **units**.\n",
    "- If the model is overfitting, try reducing the complexity, adding **dropout** layers, or increasing **regularization**.\n",
    "\n",
    "So for regression, I will create a **SimpleANN**, a **ComplexANN** and a **FinalANN** and compare them performance wise.\n",
    "The number of **epochs** and **learning rate** will be the same when comparing their performance.\n",
    "\n",
    "Same for classification, we create two models, one SimpleClassificationNN and one ComplexClassificationNN with a high number of layers and dropout."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlshit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
