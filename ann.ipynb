{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANN or Regression and Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression goal\n",
    "- We want to predict the target value streams based on certain input columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Considerations\n",
    "\n",
    "- How many hidden layers?\n",
    "- How many hidden units?\n",
    "- What kind of layers?\n",
    "- What performance is adequate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing of the data\n",
    "\n",
    "- Read data\n",
    "- One-hot-encoding\n",
    "- Remove outliers\n",
    "- Keep columns of interest\n",
    "- Normalize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`key` None count:  95\n",
      "`in_shazam_charts` None count:  50\n"
     ]
    }
   ],
   "source": [
    "dataset_path = \"dataset/dataset-spotify-2023.csv\"\n",
    "data = pd.read_csv(dataset_path, encoding=\"latin-1\")\n",
    "columns = [\"danceability_%\", \"valence_%\", \"energy_%\", \"acousticness_%\", \"instrumentalness_%\",\n",
    "\"liveness_%\", \"speechiness_%\"]\n",
    "data = data.rename(columns={column: column.replace(\"_%\", \"\") for column in columns})\n",
    "key_None_count = data[\"key\"].isna().sum()\n",
    "in_shazam_charts_None_count = data[\"in_shazam_charts\"].isna().sum()\n",
    "\n",
    "print(\"`key` None count: \", key_None_count)\n",
    "print(\"`in_shazam_charts` None count: \", in_shazam_charts_None_count)\n",
    "\n",
    "# Replace NaN values with Unspecified, it may be useful later on\n",
    "data = data.replace(np.nan, \"Unavailable\")\n",
    "\n",
    "# Data is malformed, need to remove comma `,`\n",
    "data[\"in_deezer_playlists\"] = data[\"in_deezer_playlists\"].replace(\",\", \"\", regex=True)\n",
    "data[\"in_shazam_charts\"] = data[\"in_deezer_playlists\"].replace(\",\", \"\", regex=True)\n",
    "\n",
    "# Convert columns to int64\n",
    "# streams, in_deezer_playlists, in_shazam_charts\n",
    "data[\"in_deezer_playlists\"] = data[\"in_deezer_playlists\"].astype(int)\n",
    "data[\"in_shazam_charts\"] = data[\"in_shazam_charts\"].astype(int)\n",
    "\n",
    "# Streams overflowed with int, so use np.int64 to fit the whole numbers\n",
    "data[\"streams\"] = data[\"streams\"].astype(np.int64)\n",
    "\n",
    "# Wee see that `streams` is very large compared to to other data, next larger is `in_spotify_playlists`\n",
    "# Add extra column with log value of streams\n",
    "# data[\"streams_log\"] = np.log2(data[\"streams\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding values\n",
    "data = pd.get_dummies(data, columns=[\"key\", \"mode\"], prefix=[\"key\", \"mode\"])\n",
    "data = data.applymap(lambda x: int(x) if isinstance(x, bool) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 952 entries, 0 to 951\n",
      "Data columns (total 34 columns):\n",
      " #   Column                Non-Null Count  Dtype\n",
      "---  ------                --------------  -----\n",
      " 0   artist_count          952 non-null    int64\n",
      " 1   released_year         952 non-null    int64\n",
      " 2   released_month        952 non-null    int64\n",
      " 3   released_day          952 non-null    int64\n",
      " 4   in_spotify_playlists  952 non-null    int64\n",
      " 5   in_spotify_charts     952 non-null    int64\n",
      " 6   streams               952 non-null    int64\n",
      " 7   in_apple_playlists    952 non-null    int64\n",
      " 8   in_apple_charts       952 non-null    int64\n",
      " 9   in_deezer_playlists   952 non-null    int64\n",
      " 10  in_deezer_charts      952 non-null    int64\n",
      " 11  in_shazam_charts      952 non-null    int64\n",
      " 12  bpm                   952 non-null    int64\n",
      " 13  danceability          952 non-null    int64\n",
      " 14  valence               952 non-null    int64\n",
      " 15  energy                952 non-null    int64\n",
      " 16  acousticness          952 non-null    int64\n",
      " 17  instrumentalness      952 non-null    int64\n",
      " 18  liveness              952 non-null    int64\n",
      " 19  speechiness           952 non-null    int64\n",
      " 20  key_A                 952 non-null    int64\n",
      " 21  key_A#                952 non-null    int64\n",
      " 22  key_B                 952 non-null    int64\n",
      " 23  key_C#                952 non-null    int64\n",
      " 24  key_D                 952 non-null    int64\n",
      " 25  key_D#                952 non-null    int64\n",
      " 26  key_E                 952 non-null    int64\n",
      " 27  key_F                 952 non-null    int64\n",
      " 28  key_F#                952 non-null    int64\n",
      " 29  key_G                 952 non-null    int64\n",
      " 30  key_G#                952 non-null    int64\n",
      " 31  key_Unavailable       952 non-null    int64\n",
      " 32  mode_Major            952 non-null    int64\n",
      " 33  mode_Minor            952 non-null    int64\n",
      "dtypes: int64(34)\n",
      "memory usage: 253.0 KB\n"
     ]
    }
   ],
   "source": [
    "# Select numeric columns\n",
    "data_numeric = data.select_dtypes(exclude=\"object\")\n",
    "data_numeric.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is not appropriate for now\n",
    "# z_scores = np.abs(stats.zscore(data_numeric))\n",
    "# data_numeric = data_numeric[(z_scores < 3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "scaler = StandardScaler()\n",
    "data_normalized = scaler.fit_transform(data_numeric)\n",
    "data_numeric = pd.DataFrame(data_normalized, columns=data_numeric.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selected columns:\n",
    "# Columns that express musicality and composition\n",
    "composition_features = [\"bpm\", \n",
    "                        \"mode_Major\",\n",
    "                        \"mode_Minor\",\n",
    "                        \"key_A\",\n",
    "                        \"key_A#\",\n",
    "                        \"key_B\",\n",
    "                        \"key_C#\",\n",
    "                        \"key_D\",\n",
    "                        \"key_D#\",\n",
    "                        \"key_E\",\n",
    "                        \"key_F\",\n",
    "                        \"key_F#\",\n",
    "                        \"key_G\",\n",
    "                        \"key_G#\", \n",
    "                        \"danceability\",\n",
    "                        \"valence\",\n",
    "                        \"energy\",\n",
    "                        \"acousticness\", \n",
    "                        \"instrumentalness\",\n",
    "                        \"liveness\",\n",
    "                        \"speechiness\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Less columns, because I want to cry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression\n",
    "# Target value is `streams`\n",
    "X = torch.tensor(data_numeric[composition_features].values, dtype=torch.float32)\n",
    "Y = torch.tensor(data_numeric[\"streams\"].values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create a TensorDataset with X and Y\n",
    "dataset = TensorDataset(X, Y)\n",
    "\n",
    "# Split dataset into training and validation sets\n",
    "# Let's assume we'll use 80% of the data for training and 20% for validation\n",
    "train_size = int(0.8 * len(dataset))\n",
    "valid_size = len(dataset) - train_size\n",
    "\n",
    "# Use random_split to create the training and validation subsets\n",
    "train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "# Create DataLoaders for both training and validation sets\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "input_size = X.shape[1] \n",
    "output_size = Y.shape[1] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a simple ANN\n",
    "# In total, the network is composed of two layers: one hidden layer (self.fc1 + ReLU) and one output layer (self.fc2).\n",
    "\n",
    "class SimpleANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_unit_size=16):\n",
    "        super(SimpleANN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_unit_size)\n",
    "        self.fc2 = nn.Linear(hidden_unit_size, output_size)\n",
    "\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        nn.init.xavier_uniform_(self.fc2.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a more complex ANN with more layers\n",
    "class ComplexANN(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_sizes=[32, 64]):\n",
    "        super(ComplexANN, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(input_size, hidden_sizes[0])\n",
    "        nn.init.xavier_uniform_(self.fc1.weight)\n",
    "\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for k in range(len(hidden_sizes) - 1):\n",
    "            self.hidden_layers.append(nn.Linear(hidden_sizes[k], hidden_sizes[k+1]))\n",
    "            nn.init.xavier_uniform_(self.hidden_layers[k].weight)\n",
    "\n",
    "        self.fc_out = nn.Linear(hidden_sizes[-1], output_size)\n",
    "        nn.init.xavier_uniform_(self.fc_out.weight)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        for layer in self.hidden_layers:\n",
    "            x = F.relu(layer(x))\n",
    "        \n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the finalized, parameter tuned ANN\n",
    "class FinalANN(nn.Module):\n",
    "    def __init__(self, args):\n",
    "        super(ComplexANN, self).__init__()\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of epochs\n",
    "num_epochs = 500\n",
    "# For regression, it is recommended to use MSELoss\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, criterion, optimizer, num_epochs, train_loader, valid_loader):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        total_train_loss = 0\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()  \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_train_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        total_valid_loss = 0\n",
    "        model.eval() \n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in valid_loader:\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                total_valid_loss += loss.item() * inputs.size(0)\n",
    "        \n",
    "        epoch_train_loss = total_train_loss / len(train_loader.dataset)\n",
    "        epoch_valid_loss = total_valid_loss / len(valid_loader.dataset)\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        valid_losses.append(epoch_valid_loss)\n",
    "\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_train_loss:.4f}, Validation Loss: {epoch_valid_loss:.4f}\")\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.plot(range(1, num_epochs+1), train_losses, label=\"Training Loss\")\n",
    "    plt.plot(range(1, num_epochs+1), valid_losses, label=\"Validation Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.title(\"Training and Validation Loss Over Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 1.8125, Validation Loss: 1.4569\n",
      "Epoch [2/500], Training Loss: 1.7133, Validation Loss: 1.3839\n",
      "Epoch [3/500], Training Loss: 1.6277, Validation Loss: 1.3240\n",
      "Epoch [4/500], Training Loss: 1.5535, Validation Loss: 1.2747\n",
      "Epoch [5/500], Training Loss: 1.4939, Validation Loss: 1.2319\n",
      "Epoch [6/500], Training Loss: 1.4380, Validation Loss: 1.1955\n",
      "Epoch [7/500], Training Loss: 1.3920, Validation Loss: 1.1622\n",
      "Epoch [8/500], Training Loss: 1.3502, Validation Loss: 1.1365\n",
      "Epoch [9/500], Training Loss: 1.3138, Validation Loss: 1.1124\n",
      "Epoch [10/500], Training Loss: 1.2811, Validation Loss: 1.0905\n",
      "Epoch [11/500], Training Loss: 1.2530, Validation Loss: 1.0726\n",
      "Epoch [12/500], Training Loss: 1.2279, Validation Loss: 1.0549\n",
      "Epoch [13/500], Training Loss: 1.2043, Validation Loss: 1.0401\n",
      "Epoch [14/500], Training Loss: 1.1843, Validation Loss: 1.0270\n",
      "Epoch [15/500], Training Loss: 1.1654, Validation Loss: 1.0146\n",
      "Epoch [16/500], Training Loss: 1.1493, Validation Loss: 1.0055\n",
      "Epoch [17/500], Training Loss: 1.1349, Validation Loss: 0.9961\n",
      "Epoch [18/500], Training Loss: 1.1213, Validation Loss: 0.9878\n",
      "Epoch [19/500], Training Loss: 1.1102, Validation Loss: 0.9808\n",
      "Epoch [20/500], Training Loss: 1.0991, Validation Loss: 0.9742\n",
      "Epoch [21/500], Training Loss: 1.0892, Validation Loss: 0.9685\n",
      "Epoch [22/500], Training Loss: 1.0805, Validation Loss: 0.9618\n",
      "Epoch [23/500], Training Loss: 1.0730, Validation Loss: 0.9581\n",
      "Epoch [24/500], Training Loss: 1.0656, Validation Loss: 0.9540\n",
      "Epoch [25/500], Training Loss: 1.0584, Validation Loss: 0.9500\n",
      "Epoch [26/500], Training Loss: 1.0520, Validation Loss: 0.9462\n",
      "Epoch [27/500], Training Loss: 1.0473, Validation Loss: 0.9431\n",
      "Epoch [28/500], Training Loss: 1.0417, Validation Loss: 0.9407\n",
      "Epoch [29/500], Training Loss: 1.0367, Validation Loss: 0.9389\n",
      "Epoch [30/500], Training Loss: 1.0331, Validation Loss: 0.9348\n",
      "Epoch [31/500], Training Loss: 1.0281, Validation Loss: 0.9335\n",
      "Epoch [32/500], Training Loss: 1.0239, Validation Loss: 0.9309\n",
      "Epoch [33/500], Training Loss: 1.0206, Validation Loss: 0.9301\n",
      "Epoch [34/500], Training Loss: 1.0168, Validation Loss: 0.9278\n",
      "Epoch [35/500], Training Loss: 1.0143, Validation Loss: 0.9253\n",
      "Epoch [36/500], Training Loss: 1.0105, Validation Loss: 0.9227\n",
      "Epoch [37/500], Training Loss: 1.0073, Validation Loss: 0.9216\n",
      "Epoch [38/500], Training Loss: 1.0047, Validation Loss: 0.9206\n",
      "Epoch [39/500], Training Loss: 1.0021, Validation Loss: 0.9179\n",
      "Epoch [40/500], Training Loss: 0.9997, Validation Loss: 0.9172\n",
      "Epoch [41/500], Training Loss: 0.9972, Validation Loss: 0.9175\n",
      "Epoch [42/500], Training Loss: 0.9945, Validation Loss: 0.9140\n",
      "Epoch [43/500], Training Loss: 0.9928, Validation Loss: 0.9149\n",
      "Epoch [44/500], Training Loss: 0.9901, Validation Loss: 0.9132\n",
      "Epoch [45/500], Training Loss: 0.9883, Validation Loss: 0.9114\n",
      "Epoch [46/500], Training Loss: 0.9860, Validation Loss: 0.9105\n",
      "Epoch [47/500], Training Loss: 0.9842, Validation Loss: 0.9112\n",
      "Epoch [48/500], Training Loss: 0.9821, Validation Loss: 0.9090\n",
      "Epoch [49/500], Training Loss: 0.9806, Validation Loss: 0.9083\n",
      "Epoch [50/500], Training Loss: 0.9785, Validation Loss: 0.9078\n",
      "Epoch [51/500], Training Loss: 0.9766, Validation Loss: 0.9065\n",
      "Epoch [52/500], Training Loss: 0.9749, Validation Loss: 0.9061\n",
      "Epoch [53/500], Training Loss: 0.9731, Validation Loss: 0.9055\n",
      "Epoch [54/500], Training Loss: 0.9717, Validation Loss: 0.9052\n",
      "Epoch [55/500], Training Loss: 0.9698, Validation Loss: 0.9051\n",
      "Epoch [56/500], Training Loss: 0.9682, Validation Loss: 0.9034\n",
      "Epoch [57/500], Training Loss: 0.9672, Validation Loss: 0.9032\n",
      "Epoch [58/500], Training Loss: 0.9650, Validation Loss: 0.9015\n",
      "Epoch [59/500], Training Loss: 0.9637, Validation Loss: 0.9012\n",
      "Epoch [60/500], Training Loss: 0.9627, Validation Loss: 0.9017\n",
      "Epoch [61/500], Training Loss: 0.9612, Validation Loss: 0.8994\n",
      "Epoch [62/500], Training Loss: 0.9603, Validation Loss: 0.8996\n",
      "Epoch [63/500], Training Loss: 0.9583, Validation Loss: 0.8989\n",
      "Epoch [64/500], Training Loss: 0.9579, Validation Loss: 0.8976\n",
      "Epoch [65/500], Training Loss: 0.9558, Validation Loss: 0.8974\n",
      "Epoch [66/500], Training Loss: 0.9546, Validation Loss: 0.8972\n",
      "Epoch [67/500], Training Loss: 0.9542, Validation Loss: 0.8973\n",
      "Epoch [68/500], Training Loss: 0.9521, Validation Loss: 0.8966\n",
      "Epoch [69/500], Training Loss: 0.9512, Validation Loss: 0.8951\n",
      "Epoch [70/500], Training Loss: 0.9502, Validation Loss: 0.8961\n",
      "Epoch [71/500], Training Loss: 0.9495, Validation Loss: 0.8963\n",
      "Epoch [72/500], Training Loss: 0.9476, Validation Loss: 0.8945\n",
      "Epoch [73/500], Training Loss: 0.9464, Validation Loss: 0.8944\n",
      "Epoch [74/500], Training Loss: 0.9457, Validation Loss: 0.8950\n",
      "Epoch [75/500], Training Loss: 0.9444, Validation Loss: 0.8952\n",
      "Epoch [76/500], Training Loss: 0.9428, Validation Loss: 0.8937\n",
      "Epoch [77/500], Training Loss: 0.9416, Validation Loss: 0.8930\n",
      "Epoch [78/500], Training Loss: 0.9407, Validation Loss: 0.8936\n",
      "Epoch [79/500], Training Loss: 0.9397, Validation Loss: 0.8924\n",
      "Epoch [80/500], Training Loss: 0.9384, Validation Loss: 0.8935\n",
      "Epoch [81/500], Training Loss: 0.9375, Validation Loss: 0.8915\n",
      "Epoch [82/500], Training Loss: 0.9365, Validation Loss: 0.8930\n",
      "Epoch [83/500], Training Loss: 0.9359, Validation Loss: 0.8913\n",
      "Epoch [84/500], Training Loss: 0.9351, Validation Loss: 0.8915\n",
      "Epoch [85/500], Training Loss: 0.9343, Validation Loss: 0.8919\n",
      "Epoch [86/500], Training Loss: 0.9333, Validation Loss: 0.8908\n",
      "Epoch [87/500], Training Loss: 0.9321, Validation Loss: 0.8919\n",
      "Epoch [88/500], Training Loss: 0.9314, Validation Loss: 0.8918\n",
      "Epoch [89/500], Training Loss: 0.9309, Validation Loss: 0.8900\n",
      "Epoch [90/500], Training Loss: 0.9295, Validation Loss: 0.8914\n",
      "Epoch [91/500], Training Loss: 0.9283, Validation Loss: 0.8904\n",
      "Epoch [92/500], Training Loss: 0.9280, Validation Loss: 0.8899\n",
      "Epoch [93/500], Training Loss: 0.9267, Validation Loss: 0.8899\n",
      "Epoch [94/500], Training Loss: 0.9260, Validation Loss: 0.8883\n",
      "Epoch [95/500], Training Loss: 0.9253, Validation Loss: 0.8895\n",
      "Epoch [96/500], Training Loss: 0.9241, Validation Loss: 0.8883\n",
      "Epoch [97/500], Training Loss: 0.9239, Validation Loss: 0.8885\n",
      "Epoch [98/500], Training Loss: 0.9229, Validation Loss: 0.8897\n",
      "Epoch [99/500], Training Loss: 0.9221, Validation Loss: 0.8887\n",
      "Epoch [100/500], Training Loss: 0.9209, Validation Loss: 0.8889\n",
      "Epoch [101/500], Training Loss: 0.9205, Validation Loss: 0.8892\n",
      "Epoch [102/500], Training Loss: 0.9196, Validation Loss: 0.8881\n",
      "Epoch [103/500], Training Loss: 0.9190, Validation Loss: 0.8878\n",
      "Epoch [104/500], Training Loss: 0.9179, Validation Loss: 0.8874\n",
      "Epoch [105/500], Training Loss: 0.9172, Validation Loss: 0.8878\n",
      "Epoch [106/500], Training Loss: 0.9164, Validation Loss: 0.8887\n",
      "Epoch [107/500], Training Loss: 0.9160, Validation Loss: 0.8880\n",
      "Epoch [108/500], Training Loss: 0.9151, Validation Loss: 0.8878\n",
      "Epoch [109/500], Training Loss: 0.9140, Validation Loss: 0.8869\n",
      "Epoch [110/500], Training Loss: 0.9132, Validation Loss: 0.8865\n",
      "Epoch [111/500], Training Loss: 0.9125, Validation Loss: 0.8856\n",
      "Epoch [112/500], Training Loss: 0.9119, Validation Loss: 0.8870\n",
      "Epoch [113/500], Training Loss: 0.9108, Validation Loss: 0.8873\n",
      "Epoch [114/500], Training Loss: 0.9105, Validation Loss: 0.8877\n",
      "Epoch [115/500], Training Loss: 0.9092, Validation Loss: 0.8864\n",
      "Epoch [116/500], Training Loss: 0.9092, Validation Loss: 0.8860\n",
      "Epoch [117/500], Training Loss: 0.9084, Validation Loss: 0.8854\n",
      "Epoch [118/500], Training Loss: 0.9080, Validation Loss: 0.8867\n",
      "Epoch [119/500], Training Loss: 0.9064, Validation Loss: 0.8863\n",
      "Epoch [120/500], Training Loss: 0.9063, Validation Loss: 0.8872\n",
      "Epoch [121/500], Training Loss: 0.9053, Validation Loss: 0.8859\n",
      "Epoch [122/500], Training Loss: 0.9044, Validation Loss: 0.8855\n",
      "Epoch [123/500], Training Loss: 0.9043, Validation Loss: 0.8862\n",
      "Epoch [124/500], Training Loss: 0.9034, Validation Loss: 0.8865\n",
      "Epoch [125/500], Training Loss: 0.9022, Validation Loss: 0.8853\n",
      "Epoch [126/500], Training Loss: 0.9016, Validation Loss: 0.8848\n",
      "Epoch [127/500], Training Loss: 0.9013, Validation Loss: 0.8837\n",
      "Epoch [128/500], Training Loss: 0.9003, Validation Loss: 0.8847\n",
      "Epoch [129/500], Training Loss: 0.9000, Validation Loss: 0.8854\n",
      "Epoch [130/500], Training Loss: 0.8994, Validation Loss: 0.8853\n",
      "Epoch [131/500], Training Loss: 0.8989, Validation Loss: 0.8852\n",
      "Epoch [132/500], Training Loss: 0.8984, Validation Loss: 0.8855\n",
      "Epoch [133/500], Training Loss: 0.8975, Validation Loss: 0.8868\n",
      "Epoch [134/500], Training Loss: 0.8968, Validation Loss: 0.8847\n",
      "Epoch [135/500], Training Loss: 0.8965, Validation Loss: 0.8843\n",
      "Epoch [136/500], Training Loss: 0.8954, Validation Loss: 0.8846\n",
      "Epoch [137/500], Training Loss: 0.8950, Validation Loss: 0.8855\n",
      "Epoch [138/500], Training Loss: 0.8942, Validation Loss: 0.8864\n",
      "Epoch [139/500], Training Loss: 0.8943, Validation Loss: 0.8857\n",
      "Epoch [140/500], Training Loss: 0.8938, Validation Loss: 0.8855\n",
      "Epoch [141/500], Training Loss: 0.8929, Validation Loss: 0.8861\n",
      "Epoch [142/500], Training Loss: 0.8925, Validation Loss: 0.8866\n",
      "Epoch [143/500], Training Loss: 0.8920, Validation Loss: 0.8857\n",
      "Epoch [144/500], Training Loss: 0.8912, Validation Loss: 0.8857\n",
      "Epoch [145/500], Training Loss: 0.8908, Validation Loss: 0.8865\n",
      "Epoch [146/500], Training Loss: 0.8902, Validation Loss: 0.8866\n",
      "Epoch [147/500], Training Loss: 0.8899, Validation Loss: 0.8865\n",
      "Epoch [148/500], Training Loss: 0.8892, Validation Loss: 0.8862\n",
      "Epoch [149/500], Training Loss: 0.8884, Validation Loss: 0.8862\n",
      "Epoch [150/500], Training Loss: 0.8884, Validation Loss: 0.8871\n",
      "Epoch [151/500], Training Loss: 0.8874, Validation Loss: 0.8866\n",
      "Epoch [152/500], Training Loss: 0.8876, Validation Loss: 0.8862\n",
      "Epoch [153/500], Training Loss: 0.8869, Validation Loss: 0.8872\n",
      "Epoch [154/500], Training Loss: 0.8860, Validation Loss: 0.8869\n",
      "Epoch [155/500], Training Loss: 0.8854, Validation Loss: 0.8874\n",
      "Epoch [156/500], Training Loss: 0.8848, Validation Loss: 0.8874\n",
      "Epoch [157/500], Training Loss: 0.8843, Validation Loss: 0.8871\n",
      "Epoch [158/500], Training Loss: 0.8832, Validation Loss: 0.8884\n",
      "Epoch [159/500], Training Loss: 0.8837, Validation Loss: 0.8890\n",
      "Epoch [160/500], Training Loss: 0.8824, Validation Loss: 0.8870\n",
      "Epoch [161/500], Training Loss: 0.8821, Validation Loss: 0.8885\n",
      "Epoch [162/500], Training Loss: 0.8812, Validation Loss: 0.8886\n",
      "Epoch [163/500], Training Loss: 0.8802, Validation Loss: 0.8897\n",
      "Epoch [164/500], Training Loss: 0.8799, Validation Loss: 0.8887\n",
      "Epoch [165/500], Training Loss: 0.8798, Validation Loss: 0.8888\n",
      "Epoch [166/500], Training Loss: 0.8789, Validation Loss: 0.8884\n",
      "Epoch [167/500], Training Loss: 0.8786, Validation Loss: 0.8887\n",
      "Epoch [168/500], Training Loss: 0.8783, Validation Loss: 0.8892\n",
      "Epoch [169/500], Training Loss: 0.8773, Validation Loss: 0.8883\n",
      "Epoch [170/500], Training Loss: 0.8766, Validation Loss: 0.8880\n",
      "Epoch [171/500], Training Loss: 0.8767, Validation Loss: 0.8897\n",
      "Epoch [172/500], Training Loss: 0.8757, Validation Loss: 0.8891\n",
      "Epoch [173/500], Training Loss: 0.8761, Validation Loss: 0.8892\n",
      "Epoch [174/500], Training Loss: 0.8749, Validation Loss: 0.8905\n",
      "Epoch [175/500], Training Loss: 0.8744, Validation Loss: 0.8898\n",
      "Epoch [176/500], Training Loss: 0.8741, Validation Loss: 0.8919\n",
      "Epoch [177/500], Training Loss: 0.8739, Validation Loss: 0.8910\n",
      "Epoch [178/500], Training Loss: 0.8728, Validation Loss: 0.8910\n",
      "Epoch [179/500], Training Loss: 0.8724, Validation Loss: 0.8906\n",
      "Epoch [180/500], Training Loss: 0.8715, Validation Loss: 0.8915\n",
      "Epoch [181/500], Training Loss: 0.8713, Validation Loss: 0.8917\n",
      "Epoch [182/500], Training Loss: 0.8711, Validation Loss: 0.8924\n",
      "Epoch [183/500], Training Loss: 0.8703, Validation Loss: 0.8924\n",
      "Epoch [184/500], Training Loss: 0.8698, Validation Loss: 0.8922\n",
      "Epoch [185/500], Training Loss: 0.8691, Validation Loss: 0.8910\n",
      "Epoch [186/500], Training Loss: 0.8686, Validation Loss: 0.8919\n",
      "Epoch [187/500], Training Loss: 0.8684, Validation Loss: 0.8915\n",
      "Epoch [188/500], Training Loss: 0.8675, Validation Loss: 0.8918\n",
      "Epoch [189/500], Training Loss: 0.8669, Validation Loss: 0.8909\n",
      "Epoch [190/500], Training Loss: 0.8661, Validation Loss: 0.8921\n",
      "Epoch [191/500], Training Loss: 0.8654, Validation Loss: 0.8928\n",
      "Epoch [192/500], Training Loss: 0.8649, Validation Loss: 0.8933\n",
      "Epoch [193/500], Training Loss: 0.8648, Validation Loss: 0.8937\n",
      "Epoch [194/500], Training Loss: 0.8640, Validation Loss: 0.8932\n",
      "Epoch [195/500], Training Loss: 0.8638, Validation Loss: 0.8936\n",
      "Epoch [196/500], Training Loss: 0.8628, Validation Loss: 0.8940\n",
      "Epoch [197/500], Training Loss: 0.8626, Validation Loss: 0.8932\n",
      "Epoch [198/500], Training Loss: 0.8620, Validation Loss: 0.8948\n",
      "Epoch [199/500], Training Loss: 0.8612, Validation Loss: 0.8949\n",
      "Epoch [200/500], Training Loss: 0.8610, Validation Loss: 0.8948\n",
      "Epoch [201/500], Training Loss: 0.8607, Validation Loss: 0.8951\n",
      "Epoch [202/500], Training Loss: 0.8596, Validation Loss: 0.8943\n",
      "Epoch [203/500], Training Loss: 0.8595, Validation Loss: 0.8949\n",
      "Epoch [204/500], Training Loss: 0.8587, Validation Loss: 0.8948\n",
      "Epoch [205/500], Training Loss: 0.8585, Validation Loss: 0.8949\n",
      "Epoch [206/500], Training Loss: 0.8576, Validation Loss: 0.8955\n",
      "Epoch [207/500], Training Loss: 0.8573, Validation Loss: 0.8960\n",
      "Epoch [208/500], Training Loss: 0.8573, Validation Loss: 0.8964\n",
      "Epoch [209/500], Training Loss: 0.8562, Validation Loss: 0.8960\n",
      "Epoch [210/500], Training Loss: 0.8556, Validation Loss: 0.8947\n",
      "Epoch [211/500], Training Loss: 0.8549, Validation Loss: 0.8955\n",
      "Epoch [212/500], Training Loss: 0.8547, Validation Loss: 0.8961\n",
      "Epoch [213/500], Training Loss: 0.8540, Validation Loss: 0.8964\n",
      "Epoch [214/500], Training Loss: 0.8533, Validation Loss: 0.8945\n",
      "Epoch [215/500], Training Loss: 0.8532, Validation Loss: 0.8964\n",
      "Epoch [216/500], Training Loss: 0.8529, Validation Loss: 0.8968\n",
      "Epoch [217/500], Training Loss: 0.8519, Validation Loss: 0.8967\n",
      "Epoch [218/500], Training Loss: 0.8517, Validation Loss: 0.8961\n",
      "Epoch [219/500], Training Loss: 0.8518, Validation Loss: 0.8972\n",
      "Epoch [220/500], Training Loss: 0.8505, Validation Loss: 0.8956\n",
      "Epoch [221/500], Training Loss: 0.8505, Validation Loss: 0.8950\n",
      "Epoch [222/500], Training Loss: 0.8496, Validation Loss: 0.8960\n",
      "Epoch [223/500], Training Loss: 0.8489, Validation Loss: 0.8964\n",
      "Epoch [224/500], Training Loss: 0.8493, Validation Loss: 0.8958\n",
      "Epoch [225/500], Training Loss: 0.8485, Validation Loss: 0.8965\n",
      "Epoch [226/500], Training Loss: 0.8480, Validation Loss: 0.8946\n",
      "Epoch [227/500], Training Loss: 0.8474, Validation Loss: 0.8958\n",
      "Epoch [228/500], Training Loss: 0.8470, Validation Loss: 0.8957\n",
      "Epoch [229/500], Training Loss: 0.8462, Validation Loss: 0.8960\n",
      "Epoch [230/500], Training Loss: 0.8458, Validation Loss: 0.8970\n",
      "Epoch [231/500], Training Loss: 0.8453, Validation Loss: 0.8951\n",
      "Epoch [232/500], Training Loss: 0.8454, Validation Loss: 0.8971\n",
      "Epoch [233/500], Training Loss: 0.8445, Validation Loss: 0.8965\n",
      "Epoch [234/500], Training Loss: 0.8443, Validation Loss: 0.8973\n",
      "Epoch [235/500], Training Loss: 0.8438, Validation Loss: 0.8957\n",
      "Epoch [236/500], Training Loss: 0.8428, Validation Loss: 0.8952\n",
      "Epoch [237/500], Training Loss: 0.8426, Validation Loss: 0.8966\n",
      "Epoch [238/500], Training Loss: 0.8422, Validation Loss: 0.8964\n",
      "Epoch [239/500], Training Loss: 0.8414, Validation Loss: 0.8964\n",
      "Epoch [240/500], Training Loss: 0.8411, Validation Loss: 0.8949\n",
      "Epoch [241/500], Training Loss: 0.8408, Validation Loss: 0.8967\n",
      "Epoch [242/500], Training Loss: 0.8404, Validation Loss: 0.8954\n",
      "Epoch [243/500], Training Loss: 0.8393, Validation Loss: 0.8972\n",
      "Epoch [244/500], Training Loss: 0.8397, Validation Loss: 0.8965\n",
      "Epoch [245/500], Training Loss: 0.8396, Validation Loss: 0.8974\n",
      "Epoch [246/500], Training Loss: 0.8382, Validation Loss: 0.8970\n",
      "Epoch [247/500], Training Loss: 0.8381, Validation Loss: 0.8985\n",
      "Epoch [248/500], Training Loss: 0.8377, Validation Loss: 0.8976\n",
      "Epoch [249/500], Training Loss: 0.8373, Validation Loss: 0.8989\n",
      "Epoch [250/500], Training Loss: 0.8367, Validation Loss: 0.8965\n",
      "Epoch [251/500], Training Loss: 0.8364, Validation Loss: 0.8970\n",
      "Epoch [252/500], Training Loss: 0.8357, Validation Loss: 0.8977\n",
      "Epoch [253/500], Training Loss: 0.8353, Validation Loss: 0.8975\n",
      "Epoch [254/500], Training Loss: 0.8352, Validation Loss: 0.8970\n",
      "Epoch [255/500], Training Loss: 0.8345, Validation Loss: 0.8989\n",
      "Epoch [256/500], Training Loss: 0.8347, Validation Loss: 0.8987\n",
      "Epoch [257/500], Training Loss: 0.8350, Validation Loss: 0.8990\n",
      "Epoch [258/500], Training Loss: 0.8335, Validation Loss: 0.8975\n",
      "Epoch [259/500], Training Loss: 0.8327, Validation Loss: 0.8973\n",
      "Epoch [260/500], Training Loss: 0.8326, Validation Loss: 0.8977\n",
      "Epoch [261/500], Training Loss: 0.8321, Validation Loss: 0.8990\n",
      "Epoch [262/500], Training Loss: 0.8324, Validation Loss: 0.9006\n",
      "Epoch [263/500], Training Loss: 0.8313, Validation Loss: 0.8997\n",
      "Epoch [264/500], Training Loss: 0.8309, Validation Loss: 0.8992\n",
      "Epoch [265/500], Training Loss: 0.8306, Validation Loss: 0.8997\n",
      "Epoch [266/500], Training Loss: 0.8301, Validation Loss: 0.8995\n",
      "Epoch [267/500], Training Loss: 0.8298, Validation Loss: 0.9000\n",
      "Epoch [268/500], Training Loss: 0.8296, Validation Loss: 0.8989\n",
      "Epoch [269/500], Training Loss: 0.8288, Validation Loss: 0.9007\n",
      "Epoch [270/500], Training Loss: 0.8293, Validation Loss: 0.9009\n",
      "Epoch [271/500], Training Loss: 0.8280, Validation Loss: 0.9009\n",
      "Epoch [272/500], Training Loss: 0.8281, Validation Loss: 0.9014\n",
      "Epoch [273/500], Training Loss: 0.8279, Validation Loss: 0.9012\n",
      "Epoch [274/500], Training Loss: 0.8280, Validation Loss: 0.9010\n",
      "Epoch [275/500], Training Loss: 0.8268, Validation Loss: 0.9006\n",
      "Epoch [276/500], Training Loss: 0.8270, Validation Loss: 0.9007\n",
      "Epoch [277/500], Training Loss: 0.8262, Validation Loss: 0.9006\n",
      "Epoch [278/500], Training Loss: 0.8261, Validation Loss: 0.9010\n",
      "Epoch [279/500], Training Loss: 0.8261, Validation Loss: 0.9020\n",
      "Epoch [280/500], Training Loss: 0.8255, Validation Loss: 0.9024\n",
      "Epoch [281/500], Training Loss: 0.8251, Validation Loss: 0.9028\n",
      "Epoch [282/500], Training Loss: 0.8245, Validation Loss: 0.9022\n",
      "Epoch [283/500], Training Loss: 0.8244, Validation Loss: 0.9024\n",
      "Epoch [284/500], Training Loss: 0.8247, Validation Loss: 0.9023\n",
      "Epoch [285/500], Training Loss: 0.8240, Validation Loss: 0.9027\n",
      "Epoch [286/500], Training Loss: 0.8249, Validation Loss: 0.9021\n",
      "Epoch [287/500], Training Loss: 0.8236, Validation Loss: 0.9043\n",
      "Epoch [288/500], Training Loss: 0.8230, Validation Loss: 0.9030\n",
      "Epoch [289/500], Training Loss: 0.8223, Validation Loss: 0.9043\n",
      "Epoch [290/500], Training Loss: 0.8221, Validation Loss: 0.9043\n",
      "Epoch [291/500], Training Loss: 0.8218, Validation Loss: 0.9044\n",
      "Epoch [292/500], Training Loss: 0.8214, Validation Loss: 0.9054\n",
      "Epoch [293/500], Training Loss: 0.8212, Validation Loss: 0.9044\n",
      "Epoch [294/500], Training Loss: 0.8210, Validation Loss: 0.9046\n",
      "Epoch [295/500], Training Loss: 0.8205, Validation Loss: 0.9051\n",
      "Epoch [296/500], Training Loss: 0.8202, Validation Loss: 0.9067\n",
      "Epoch [297/500], Training Loss: 0.8200, Validation Loss: 0.9047\n",
      "Epoch [298/500], Training Loss: 0.8196, Validation Loss: 0.9051\n",
      "Epoch [299/500], Training Loss: 0.8195, Validation Loss: 0.9069\n",
      "Epoch [300/500], Training Loss: 0.8189, Validation Loss: 0.9062\n",
      "Epoch [301/500], Training Loss: 0.8190, Validation Loss: 0.9062\n",
      "Epoch [302/500], Training Loss: 0.8188, Validation Loss: 0.9059\n",
      "Epoch [303/500], Training Loss: 0.8178, Validation Loss: 0.9055\n",
      "Epoch [304/500], Training Loss: 0.8178, Validation Loss: 0.9067\n",
      "Epoch [305/500], Training Loss: 0.8173, Validation Loss: 0.9067\n",
      "Epoch [306/500], Training Loss: 0.8173, Validation Loss: 0.9071\n",
      "Epoch [307/500], Training Loss: 0.8180, Validation Loss: 0.9071\n",
      "Epoch [308/500], Training Loss: 0.8170, Validation Loss: 0.9075\n",
      "Epoch [309/500], Training Loss: 0.8164, Validation Loss: 0.9084\n",
      "Epoch [310/500], Training Loss: 0.8163, Validation Loss: 0.9071\n",
      "Epoch [311/500], Training Loss: 0.8157, Validation Loss: 0.9092\n",
      "Epoch [312/500], Training Loss: 0.8156, Validation Loss: 0.9083\n",
      "Epoch [313/500], Training Loss: 0.8152, Validation Loss: 0.9092\n",
      "Epoch [314/500], Training Loss: 0.8149, Validation Loss: 0.9111\n",
      "Epoch [315/500], Training Loss: 0.8144, Validation Loss: 0.9085\n",
      "Epoch [316/500], Training Loss: 0.8139, Validation Loss: 0.9091\n",
      "Epoch [317/500], Training Loss: 0.8141, Validation Loss: 0.9093\n",
      "Epoch [318/500], Training Loss: 0.8138, Validation Loss: 0.9094\n",
      "Epoch [319/500], Training Loss: 0.8136, Validation Loss: 0.9108\n",
      "Epoch [320/500], Training Loss: 0.8129, Validation Loss: 0.9108\n",
      "Epoch [321/500], Training Loss: 0.8129, Validation Loss: 0.9096\n",
      "Epoch [322/500], Training Loss: 0.8124, Validation Loss: 0.9109\n",
      "Epoch [323/500], Training Loss: 0.8123, Validation Loss: 0.9103\n",
      "Epoch [324/500], Training Loss: 0.8117, Validation Loss: 0.9094\n",
      "Epoch [325/500], Training Loss: 0.8118, Validation Loss: 0.9110\n",
      "Epoch [326/500], Training Loss: 0.8117, Validation Loss: 0.9112\n",
      "Epoch [327/500], Training Loss: 0.8111, Validation Loss: 0.9134\n",
      "Epoch [328/500], Training Loss: 0.8106, Validation Loss: 0.9127\n",
      "Epoch [329/500], Training Loss: 0.8105, Validation Loss: 0.9126\n",
      "Epoch [330/500], Training Loss: 0.8097, Validation Loss: 0.9112\n",
      "Epoch [331/500], Training Loss: 0.8098, Validation Loss: 0.9107\n",
      "Epoch [332/500], Training Loss: 0.8098, Validation Loss: 0.9136\n",
      "Epoch [333/500], Training Loss: 0.8090, Validation Loss: 0.9132\n",
      "Epoch [334/500], Training Loss: 0.8091, Validation Loss: 0.9142\n",
      "Epoch [335/500], Training Loss: 0.8084, Validation Loss: 0.9123\n",
      "Epoch [336/500], Training Loss: 0.8088, Validation Loss: 0.9140\n",
      "Epoch [337/500], Training Loss: 0.8079, Validation Loss: 0.9142\n",
      "Epoch [338/500], Training Loss: 0.8079, Validation Loss: 0.9136\n",
      "Epoch [339/500], Training Loss: 0.8071, Validation Loss: 0.9145\n",
      "Epoch [340/500], Training Loss: 0.8071, Validation Loss: 0.9147\n",
      "Epoch [341/500], Training Loss: 0.8067, Validation Loss: 0.9138\n",
      "Epoch [342/500], Training Loss: 0.8059, Validation Loss: 0.9138\n",
      "Epoch [343/500], Training Loss: 0.8065, Validation Loss: 0.9132\n",
      "Epoch [344/500], Training Loss: 0.8055, Validation Loss: 0.9148\n",
      "Epoch [345/500], Training Loss: 0.8060, Validation Loss: 0.9152\n"
     ]
    }
   ],
   "source": [
    "# Train model SimpleANN\n",
    "simpleANN = SimpleANN(input_size, output_size)\n",
    "# Add weight decay\n",
    "optimizer = torch.optim.Adam(simpleANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "\n",
    "train(simpleANN, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Training Loss: 1.1475, Validation Loss: 1.2151\n",
      "Epoch [2/500], Training Loss: 1.0229, Validation Loss: 1.1626\n",
      "Epoch [3/500], Training Loss: 0.9775, Validation Loss: 1.1283\n",
      "Epoch [4/500], Training Loss: 0.9477, Validation Loss: 1.1032\n",
      "Epoch [5/500], Training Loss: 0.9274, Validation Loss: 1.0974\n",
      "Epoch [6/500], Training Loss: 0.9118, Validation Loss: 1.0857\n",
      "Epoch [7/500], Training Loss: 0.8963, Validation Loss: 1.0835\n",
      "Epoch [8/500], Training Loss: 0.8864, Validation Loss: 1.0787\n",
      "Epoch [9/500], Training Loss: 0.8732, Validation Loss: 1.0790\n",
      "Epoch [10/500], Training Loss: 0.8656, Validation Loss: 1.0783\n",
      "Epoch [11/500], Training Loss: 0.8548, Validation Loss: 1.0719\n",
      "Epoch [12/500], Training Loss: 0.8477, Validation Loss: 1.0724\n",
      "Epoch [13/500], Training Loss: 0.8428, Validation Loss: 1.0704\n",
      "Epoch [14/500], Training Loss: 0.8303, Validation Loss: 1.0722\n",
      "Epoch [15/500], Training Loss: 0.8220, Validation Loss: 1.0689\n",
      "Epoch [16/500], Training Loss: 0.8157, Validation Loss: 1.0704\n",
      "Epoch [17/500], Training Loss: 0.8121, Validation Loss: 1.0712\n",
      "Epoch [18/500], Training Loss: 0.8028, Validation Loss: 1.0666\n",
      "Epoch [19/500], Training Loss: 0.7926, Validation Loss: 1.0707\n",
      "Epoch [20/500], Training Loss: 0.7858, Validation Loss: 1.0714\n",
      "Epoch [21/500], Training Loss: 0.7816, Validation Loss: 1.0736\n",
      "Epoch [22/500], Training Loss: 0.7731, Validation Loss: 1.0690\n",
      "Epoch [23/500], Training Loss: 0.7687, Validation Loss: 1.0694\n",
      "Epoch [24/500], Training Loss: 0.7599, Validation Loss: 1.0732\n",
      "Epoch [25/500], Training Loss: 0.7558, Validation Loss: 1.0787\n",
      "Epoch [26/500], Training Loss: 0.7491, Validation Loss: 1.0808\n",
      "Epoch [27/500], Training Loss: 0.7437, Validation Loss: 1.0795\n",
      "Epoch [28/500], Training Loss: 0.7383, Validation Loss: 1.0812\n",
      "Epoch [29/500], Training Loss: 0.7329, Validation Loss: 1.0838\n",
      "Epoch [30/500], Training Loss: 0.7282, Validation Loss: 1.0868\n",
      "Epoch [31/500], Training Loss: 0.7231, Validation Loss: 1.0866\n",
      "Epoch [32/500], Training Loss: 0.7177, Validation Loss: 1.0930\n",
      "Epoch [33/500], Training Loss: 0.7117, Validation Loss: 1.1005\n",
      "Epoch [34/500], Training Loss: 0.7053, Validation Loss: 1.0957\n",
      "Epoch [35/500], Training Loss: 0.7018, Validation Loss: 1.0950\n",
      "Epoch [36/500], Training Loss: 0.6977, Validation Loss: 1.1021\n",
      "Epoch [37/500], Training Loss: 0.6923, Validation Loss: 1.1040\n",
      "Epoch [38/500], Training Loss: 0.6860, Validation Loss: 1.1065\n",
      "Epoch [39/500], Training Loss: 0.6825, Validation Loss: 1.1103\n",
      "Epoch [40/500], Training Loss: 0.6768, Validation Loss: 1.1070\n",
      "Epoch [41/500], Training Loss: 0.6745, Validation Loss: 1.1195\n",
      "Epoch [42/500], Training Loss: 0.6702, Validation Loss: 1.1207\n",
      "Epoch [43/500], Training Loss: 0.6656, Validation Loss: 1.1267\n",
      "Epoch [44/500], Training Loss: 0.6621, Validation Loss: 1.1197\n",
      "Epoch [45/500], Training Loss: 0.6550, Validation Loss: 1.1256\n",
      "Epoch [46/500], Training Loss: 0.6501, Validation Loss: 1.1323\n",
      "Epoch [47/500], Training Loss: 0.6474, Validation Loss: 1.1353\n",
      "Epoch [48/500], Training Loss: 0.6403, Validation Loss: 1.1325\n",
      "Epoch [49/500], Training Loss: 0.6393, Validation Loss: 1.1369\n",
      "Epoch [50/500], Training Loss: 0.6338, Validation Loss: 1.1475\n",
      "Epoch [51/500], Training Loss: 0.6322, Validation Loss: 1.1444\n",
      "Epoch [52/500], Training Loss: 0.6265, Validation Loss: 1.1521\n",
      "Epoch [53/500], Training Loss: 0.6225, Validation Loss: 1.1593\n",
      "Epoch [54/500], Training Loss: 0.6179, Validation Loss: 1.1532\n",
      "Epoch [55/500], Training Loss: 0.6157, Validation Loss: 1.1610\n",
      "Epoch [56/500], Training Loss: 0.6078, Validation Loss: 1.1706\n",
      "Epoch [57/500], Training Loss: 0.6058, Validation Loss: 1.1749\n",
      "Epoch [58/500], Training Loss: 0.6086, Validation Loss: 1.1645\n",
      "Epoch [59/500], Training Loss: 0.5973, Validation Loss: 1.1783\n",
      "Epoch [60/500], Training Loss: 0.5923, Validation Loss: 1.1797\n",
      "Epoch [61/500], Training Loss: 0.5890, Validation Loss: 1.1851\n",
      "Epoch [62/500], Training Loss: 0.5859, Validation Loss: 1.1824\n",
      "Epoch [63/500], Training Loss: 0.5822, Validation Loss: 1.1898\n",
      "Epoch [64/500], Training Loss: 0.5764, Validation Loss: 1.1994\n",
      "Epoch [65/500], Training Loss: 0.5742, Validation Loss: 1.1928\n",
      "Epoch [66/500], Training Loss: 0.5713, Validation Loss: 1.2134\n",
      "Epoch [67/500], Training Loss: 0.5668, Validation Loss: 1.2147\n",
      "Epoch [68/500], Training Loss: 0.5631, Validation Loss: 1.2147\n",
      "Epoch [69/500], Training Loss: 0.5576, Validation Loss: 1.2199\n",
      "Epoch [70/500], Training Loss: 0.5562, Validation Loss: 1.2179\n",
      "Epoch [71/500], Training Loss: 0.5568, Validation Loss: 1.2194\n",
      "Epoch [72/500], Training Loss: 0.5568, Validation Loss: 1.2356\n",
      "Epoch [73/500], Training Loss: 0.5445, Validation Loss: 1.2285\n",
      "Epoch [74/500], Training Loss: 0.5455, Validation Loss: 1.2275\n",
      "Epoch [75/500], Training Loss: 0.5373, Validation Loss: 1.2384\n",
      "Epoch [76/500], Training Loss: 0.5368, Validation Loss: 1.2358\n",
      "Epoch [77/500], Training Loss: 0.5324, Validation Loss: 1.2404\n",
      "Epoch [78/500], Training Loss: 0.5251, Validation Loss: 1.2448\n",
      "Epoch [79/500], Training Loss: 0.5238, Validation Loss: 1.2528\n",
      "Epoch [80/500], Training Loss: 0.5223, Validation Loss: 1.2563\n",
      "Epoch [81/500], Training Loss: 0.5156, Validation Loss: 1.2517\n",
      "Epoch [82/500], Training Loss: 0.5125, Validation Loss: 1.2615\n",
      "Epoch [83/500], Training Loss: 0.5084, Validation Loss: 1.2671\n",
      "Epoch [84/500], Training Loss: 0.5068, Validation Loss: 1.2678\n",
      "Epoch [85/500], Training Loss: 0.5036, Validation Loss: 1.2629\n",
      "Epoch [86/500], Training Loss: 0.5016, Validation Loss: 1.2866\n",
      "Epoch [87/500], Training Loss: 0.4947, Validation Loss: 1.2732\n",
      "Epoch [88/500], Training Loss: 0.4946, Validation Loss: 1.2725\n",
      "Epoch [89/500], Training Loss: 0.4922, Validation Loss: 1.2762\n",
      "Epoch [90/500], Training Loss: 0.4876, Validation Loss: 1.3054\n",
      "Epoch [91/500], Training Loss: 0.4880, Validation Loss: 1.2898\n",
      "Epoch [92/500], Training Loss: 0.4782, Validation Loss: 1.3014\n",
      "Epoch [93/500], Training Loss: 0.4786, Validation Loss: 1.2966\n",
      "Epoch [94/500], Training Loss: 0.4742, Validation Loss: 1.2956\n",
      "Epoch [95/500], Training Loss: 0.4708, Validation Loss: 1.3138\n",
      "Epoch [96/500], Training Loss: 0.4659, Validation Loss: 1.3041\n",
      "Epoch [97/500], Training Loss: 0.4634, Validation Loss: 1.3128\n",
      "Epoch [98/500], Training Loss: 0.4604, Validation Loss: 1.3185\n",
      "Epoch [99/500], Training Loss: 0.4577, Validation Loss: 1.3164\n",
      "Epoch [100/500], Training Loss: 0.4563, Validation Loss: 1.3119\n",
      "Epoch [101/500], Training Loss: 0.4504, Validation Loss: 1.3265\n",
      "Epoch [102/500], Training Loss: 0.4505, Validation Loss: 1.3262\n",
      "Epoch [103/500], Training Loss: 0.4478, Validation Loss: 1.3062\n",
      "Epoch [104/500], Training Loss: 0.4459, Validation Loss: 1.3332\n",
      "Epoch [105/500], Training Loss: 0.4426, Validation Loss: 1.3383\n",
      "Epoch [106/500], Training Loss: 0.4358, Validation Loss: 1.3277\n",
      "Epoch [107/500], Training Loss: 0.4346, Validation Loss: 1.3338\n",
      "Epoch [108/500], Training Loss: 0.4311, Validation Loss: 1.3455\n",
      "Epoch [109/500], Training Loss: 0.4292, Validation Loss: 1.3358\n",
      "Epoch [110/500], Training Loss: 0.4248, Validation Loss: 1.3404\n",
      "Epoch [111/500], Training Loss: 0.4257, Validation Loss: 1.3407\n",
      "Epoch [112/500], Training Loss: 0.4190, Validation Loss: 1.3480\n",
      "Epoch [113/500], Training Loss: 0.4168, Validation Loss: 1.3496\n",
      "Epoch [114/500], Training Loss: 0.4162, Validation Loss: 1.3690\n",
      "Epoch [115/500], Training Loss: 0.4167, Validation Loss: 1.3434\n",
      "Epoch [116/500], Training Loss: 0.4101, Validation Loss: 1.3557\n",
      "Epoch [117/500], Training Loss: 0.4070, Validation Loss: 1.3541\n",
      "Epoch [118/500], Training Loss: 0.4036, Validation Loss: 1.3599\n",
      "Epoch [119/500], Training Loss: 0.4016, Validation Loss: 1.3661\n",
      "Epoch [120/500], Training Loss: 0.3978, Validation Loss: 1.3682\n",
      "Epoch [121/500], Training Loss: 0.3960, Validation Loss: 1.3619\n",
      "Epoch [122/500], Training Loss: 0.3927, Validation Loss: 1.3680\n",
      "Epoch [123/500], Training Loss: 0.3904, Validation Loss: 1.3675\n",
      "Epoch [124/500], Training Loss: 0.3877, Validation Loss: 1.3829\n",
      "Epoch [125/500], Training Loss: 0.3843, Validation Loss: 1.3782\n",
      "Epoch [126/500], Training Loss: 0.3813, Validation Loss: 1.3749\n",
      "Epoch [127/500], Training Loss: 0.3796, Validation Loss: 1.3874\n",
      "Epoch [128/500], Training Loss: 0.3756, Validation Loss: 1.3832\n",
      "Epoch [129/500], Training Loss: 0.3747, Validation Loss: 1.3826\n",
      "Epoch [130/500], Training Loss: 0.3701, Validation Loss: 1.3880\n",
      "Epoch [131/500], Training Loss: 0.3676, Validation Loss: 1.3852\n",
      "Epoch [132/500], Training Loss: 0.3678, Validation Loss: 1.3915\n",
      "Epoch [133/500], Training Loss: 0.3616, Validation Loss: 1.3912\n",
      "Epoch [134/500], Training Loss: 0.3627, Validation Loss: 1.4035\n",
      "Epoch [135/500], Training Loss: 0.3590, Validation Loss: 1.3947\n",
      "Epoch [136/500], Training Loss: 0.3578, Validation Loss: 1.4134\n",
      "Epoch [137/500], Training Loss: 0.3558, Validation Loss: 1.3941\n",
      "Epoch [138/500], Training Loss: 0.3517, Validation Loss: 1.3948\n",
      "Epoch [139/500], Training Loss: 0.3488, Validation Loss: 1.4143\n",
      "Epoch [140/500], Training Loss: 0.3499, Validation Loss: 1.3995\n",
      "Epoch [141/500], Training Loss: 0.3445, Validation Loss: 1.4211\n",
      "Epoch [142/500], Training Loss: 0.3455, Validation Loss: 1.4220\n",
      "Epoch [143/500], Training Loss: 0.3402, Validation Loss: 1.4167\n",
      "Epoch [144/500], Training Loss: 0.3372, Validation Loss: 1.4205\n",
      "Epoch [145/500], Training Loss: 0.3373, Validation Loss: 1.4264\n",
      "Epoch [146/500], Training Loss: 0.3332, Validation Loss: 1.4209\n",
      "Epoch [147/500], Training Loss: 0.3294, Validation Loss: 1.4187\n",
      "Epoch [148/500], Training Loss: 0.3280, Validation Loss: 1.4304\n",
      "Epoch [149/500], Training Loss: 0.3262, Validation Loss: 1.4207\n",
      "Epoch [150/500], Training Loss: 0.3258, Validation Loss: 1.4264\n",
      "Epoch [151/500], Training Loss: 0.3208, Validation Loss: 1.4330\n",
      "Epoch [152/500], Training Loss: 0.3191, Validation Loss: 1.4381\n",
      "Epoch [153/500], Training Loss: 0.3167, Validation Loss: 1.4302\n",
      "Epoch [154/500], Training Loss: 0.3197, Validation Loss: 1.4402\n",
      "Epoch [155/500], Training Loss: 0.3131, Validation Loss: 1.4295\n",
      "Epoch [156/500], Training Loss: 0.3121, Validation Loss: 1.4648\n",
      "Epoch [157/500], Training Loss: 0.3117, Validation Loss: 1.4440\n",
      "Epoch [158/500], Training Loss: 0.3063, Validation Loss: 1.4532\n",
      "Epoch [159/500], Training Loss: 0.3036, Validation Loss: 1.4536\n",
      "Epoch [160/500], Training Loss: 0.3021, Validation Loss: 1.4552\n",
      "Epoch [161/500], Training Loss: 0.3007, Validation Loss: 1.4543\n",
      "Epoch [162/500], Training Loss: 0.2984, Validation Loss: 1.4509\n",
      "Epoch [163/500], Training Loss: 0.3004, Validation Loss: 1.4542\n",
      "Epoch [164/500], Training Loss: 0.2942, Validation Loss: 1.4524\n",
      "Epoch [165/500], Training Loss: 0.2951, Validation Loss: 1.4536\n",
      "Epoch [166/500], Training Loss: 0.2915, Validation Loss: 1.4504\n",
      "Epoch [167/500], Training Loss: 0.2909, Validation Loss: 1.4802\n",
      "Epoch [168/500], Training Loss: 0.2906, Validation Loss: 1.4653\n",
      "Epoch [169/500], Training Loss: 0.2888, Validation Loss: 1.4758\n",
      "Epoch [170/500], Training Loss: 0.2864, Validation Loss: 1.4700\n",
      "Epoch [171/500], Training Loss: 0.2826, Validation Loss: 1.4677\n",
      "Epoch [172/500], Training Loss: 0.2787, Validation Loss: 1.4692\n",
      "Epoch [173/500], Training Loss: 0.2757, Validation Loss: 1.4791\n",
      "Epoch [174/500], Training Loss: 0.2782, Validation Loss: 1.4742\n",
      "Epoch [175/500], Training Loss: 0.2793, Validation Loss: 1.4799\n",
      "Epoch [176/500], Training Loss: 0.2780, Validation Loss: 1.4697\n",
      "Epoch [177/500], Training Loss: 0.2697, Validation Loss: 1.4800\n",
      "Epoch [178/500], Training Loss: 0.2672, Validation Loss: 1.4812\n",
      "Epoch [179/500], Training Loss: 0.2664, Validation Loss: 1.4870\n",
      "Epoch [180/500], Training Loss: 0.2658, Validation Loss: 1.5009\n",
      "Epoch [181/500], Training Loss: 0.2631, Validation Loss: 1.4874\n",
      "Epoch [182/500], Training Loss: 0.2603, Validation Loss: 1.4962\n",
      "Epoch [183/500], Training Loss: 0.2596, Validation Loss: 1.4866\n",
      "Epoch [184/500], Training Loss: 0.2602, Validation Loss: 1.4963\n",
      "Epoch [185/500], Training Loss: 0.2582, Validation Loss: 1.4879\n",
      "Epoch [186/500], Training Loss: 0.2590, Validation Loss: 1.5015\n",
      "Epoch [187/500], Training Loss: 0.2543, Validation Loss: 1.4963\n",
      "Epoch [188/500], Training Loss: 0.2513, Validation Loss: 1.4946\n",
      "Epoch [189/500], Training Loss: 0.2486, Validation Loss: 1.5014\n",
      "Epoch [190/500], Training Loss: 0.2452, Validation Loss: 1.5079\n",
      "Epoch [191/500], Training Loss: 0.2522, Validation Loss: 1.5204\n",
      "Epoch [192/500], Training Loss: 0.2474, Validation Loss: 1.5187\n",
      "Epoch [193/500], Training Loss: 0.2468, Validation Loss: 1.5206\n",
      "Epoch [194/500], Training Loss: 0.2421, Validation Loss: 1.5149\n",
      "Epoch [195/500], Training Loss: 0.2399, Validation Loss: 1.5066\n",
      "Epoch [196/500], Training Loss: 0.2377, Validation Loss: 1.5055\n",
      "Epoch [197/500], Training Loss: 0.2364, Validation Loss: 1.5137\n",
      "Epoch [198/500], Training Loss: 0.2368, Validation Loss: 1.5201\n",
      "Epoch [199/500], Training Loss: 0.2344, Validation Loss: 1.5226\n",
      "Epoch [200/500], Training Loss: 0.2327, Validation Loss: 1.5157\n",
      "Epoch [201/500], Training Loss: 0.2306, Validation Loss: 1.5230\n",
      "Epoch [202/500], Training Loss: 0.2282, Validation Loss: 1.5285\n",
      "Epoch [203/500], Training Loss: 0.2307, Validation Loss: 1.5224\n",
      "Epoch [204/500], Training Loss: 0.2224, Validation Loss: 1.5369\n",
      "Epoch [205/500], Training Loss: 0.2243, Validation Loss: 1.5251\n",
      "Epoch [206/500], Training Loss: 0.2222, Validation Loss: 1.5181\n",
      "Epoch [207/500], Training Loss: 0.2202, Validation Loss: 1.5370\n",
      "Epoch [208/500], Training Loss: 0.2212, Validation Loss: 1.5285\n",
      "Epoch [209/500], Training Loss: 0.2176, Validation Loss: 1.5479\n",
      "Epoch [210/500], Training Loss: 0.2187, Validation Loss: 1.5539\n",
      "Epoch [211/500], Training Loss: 0.2161, Validation Loss: 1.5477\n",
      "Epoch [212/500], Training Loss: 0.2129, Validation Loss: 1.5357\n",
      "Epoch [213/500], Training Loss: 0.2112, Validation Loss: 1.5571\n",
      "Epoch [214/500], Training Loss: 0.2098, Validation Loss: 1.5402\n",
      "Epoch [215/500], Training Loss: 0.2131, Validation Loss: 1.5417\n",
      "Epoch [216/500], Training Loss: 0.2088, Validation Loss: 1.5490\n",
      "Epoch [217/500], Training Loss: 0.2041, Validation Loss: 1.5374\n",
      "Epoch [218/500], Training Loss: 0.2038, Validation Loss: 1.5513\n",
      "Epoch [219/500], Training Loss: 0.2044, Validation Loss: 1.5509\n",
      "Epoch [220/500], Training Loss: 0.2014, Validation Loss: 1.5479\n",
      "Epoch [221/500], Training Loss: 0.1994, Validation Loss: 1.5588\n",
      "Epoch [222/500], Training Loss: 0.1979, Validation Loss: 1.5589\n",
      "Epoch [223/500], Training Loss: 0.1996, Validation Loss: 1.5601\n",
      "Epoch [224/500], Training Loss: 0.1968, Validation Loss: 1.5657\n",
      "Epoch [225/500], Training Loss: 0.1954, Validation Loss: 1.5720\n",
      "Epoch [226/500], Training Loss: 0.1939, Validation Loss: 1.5651\n",
      "Epoch [227/500], Training Loss: 0.1920, Validation Loss: 1.5655\n",
      "Epoch [228/500], Training Loss: 0.1910, Validation Loss: 1.5680\n",
      "Epoch [229/500], Training Loss: 0.1908, Validation Loss: 1.5708\n",
      "Epoch [230/500], Training Loss: 0.1892, Validation Loss: 1.5916\n",
      "Epoch [231/500], Training Loss: 0.1897, Validation Loss: 1.5708\n",
      "Epoch [232/500], Training Loss: 0.1849, Validation Loss: 1.5867\n",
      "Epoch [233/500], Training Loss: 0.1838, Validation Loss: 1.5800\n",
      "Epoch [234/500], Training Loss: 0.1842, Validation Loss: 1.5698\n",
      "Epoch [235/500], Training Loss: 0.1822, Validation Loss: 1.5943\n",
      "Epoch [236/500], Training Loss: 0.1828, Validation Loss: 1.5746\n",
      "Epoch [237/500], Training Loss: 0.1787, Validation Loss: 1.5899\n",
      "Epoch [238/500], Training Loss: 0.1771, Validation Loss: 1.5884\n",
      "Epoch [239/500], Training Loss: 0.1780, Validation Loss: 1.6070\n",
      "Epoch [240/500], Training Loss: 0.1771, Validation Loss: 1.5907\n",
      "Epoch [241/500], Training Loss: 0.1752, Validation Loss: 1.5932\n",
      "Epoch [242/500], Training Loss: 0.1741, Validation Loss: 1.6127\n",
      "Epoch [243/500], Training Loss: 0.1744, Validation Loss: 1.6024\n",
      "Epoch [244/500], Training Loss: 0.1742, Validation Loss: 1.6030\n",
      "Epoch [245/500], Training Loss: 0.1720, Validation Loss: 1.5989\n",
      "Epoch [246/500], Training Loss: 0.1687, Validation Loss: 1.6019\n",
      "Epoch [247/500], Training Loss: 0.1696, Validation Loss: 1.5913\n",
      "Epoch [248/500], Training Loss: 0.1678, Validation Loss: 1.6097\n",
      "Epoch [249/500], Training Loss: 0.1658, Validation Loss: 1.6134\n",
      "Epoch [250/500], Training Loss: 0.1652, Validation Loss: 1.6120\n",
      "Epoch [251/500], Training Loss: 0.1641, Validation Loss: 1.6232\n",
      "Epoch [252/500], Training Loss: 0.1618, Validation Loss: 1.6104\n",
      "Epoch [253/500], Training Loss: 0.1613, Validation Loss: 1.6171\n",
      "Epoch [254/500], Training Loss: 0.1601, Validation Loss: 1.6302\n",
      "Epoch [255/500], Training Loss: 0.1579, Validation Loss: 1.6168\n",
      "Epoch [256/500], Training Loss: 0.1596, Validation Loss: 1.6254\n",
      "Epoch [257/500], Training Loss: 0.1566, Validation Loss: 1.6320\n",
      "Epoch [258/500], Training Loss: 0.1553, Validation Loss: 1.6407\n",
      "Epoch [259/500], Training Loss: 0.1534, Validation Loss: 1.6317\n",
      "Epoch [260/500], Training Loss: 0.1526, Validation Loss: 1.6527\n",
      "Epoch [261/500], Training Loss: 0.1555, Validation Loss: 1.6338\n",
      "Epoch [262/500], Training Loss: 0.1554, Validation Loss: 1.6434\n",
      "Epoch [263/500], Training Loss: 0.1529, Validation Loss: 1.6419\n",
      "Epoch [264/500], Training Loss: 0.1513, Validation Loss: 1.6347\n",
      "Epoch [265/500], Training Loss: 0.1478, Validation Loss: 1.6423\n",
      "Epoch [266/500], Training Loss: 0.1486, Validation Loss: 1.6445\n",
      "Epoch [267/500], Training Loss: 0.1463, Validation Loss: 1.6517\n",
      "Epoch [268/500], Training Loss: 0.1467, Validation Loss: 1.6610\n",
      "Epoch [269/500], Training Loss: 0.1435, Validation Loss: 1.6491\n",
      "Epoch [270/500], Training Loss: 0.1410, Validation Loss: 1.6489\n",
      "Epoch [271/500], Training Loss: 0.1414, Validation Loss: 1.6591\n",
      "Epoch [272/500], Training Loss: 0.1407, Validation Loss: 1.6574\n",
      "Epoch [273/500], Training Loss: 0.1403, Validation Loss: 1.6761\n",
      "Epoch [274/500], Training Loss: 0.1405, Validation Loss: 1.6577\n",
      "Epoch [275/500], Training Loss: 0.1389, Validation Loss: 1.6769\n",
      "Epoch [276/500], Training Loss: 0.1367, Validation Loss: 1.6666\n",
      "Epoch [277/500], Training Loss: 0.1352, Validation Loss: 1.6697\n",
      "Epoch [278/500], Training Loss: 0.1367, Validation Loss: 1.6757\n",
      "Epoch [279/500], Training Loss: 0.1334, Validation Loss: 1.6797\n",
      "Epoch [280/500], Training Loss: 0.1317, Validation Loss: 1.6600\n",
      "Epoch [281/500], Training Loss: 0.1339, Validation Loss: 1.6809\n",
      "Epoch [282/500], Training Loss: 0.1333, Validation Loss: 1.6891\n",
      "Epoch [283/500], Training Loss: 0.1308, Validation Loss: 1.6779\n",
      "Epoch [284/500], Training Loss: 0.1328, Validation Loss: 1.6789\n",
      "Epoch [285/500], Training Loss: 0.1295, Validation Loss: 1.6815\n",
      "Epoch [286/500], Training Loss: 0.1276, Validation Loss: 1.7035\n",
      "Epoch [287/500], Training Loss: 0.1271, Validation Loss: 1.6756\n",
      "Epoch [288/500], Training Loss: 0.1261, Validation Loss: 1.6989\n",
      "Epoch [289/500], Training Loss: 0.1251, Validation Loss: 1.6939\n",
      "Epoch [290/500], Training Loss: 0.1245, Validation Loss: 1.6965\n",
      "Epoch [291/500], Training Loss: 0.1242, Validation Loss: 1.6866\n",
      "Epoch [292/500], Training Loss: 0.1219, Validation Loss: 1.6983\n",
      "Epoch [293/500], Training Loss: 0.1232, Validation Loss: 1.7013\n",
      "Epoch [294/500], Training Loss: 0.1210, Validation Loss: 1.7030\n",
      "Epoch [295/500], Training Loss: 0.1210, Validation Loss: 1.6954\n",
      "Epoch [296/500], Training Loss: 0.1188, Validation Loss: 1.7268\n",
      "Epoch [297/500], Training Loss: 0.1201, Validation Loss: 1.6869\n",
      "Epoch [298/500], Training Loss: 0.1175, Validation Loss: 1.7157\n",
      "Epoch [299/500], Training Loss: 0.1169, Validation Loss: 1.7025\n",
      "Epoch [300/500], Training Loss: 0.1168, Validation Loss: 1.7308\n",
      "Epoch [301/500], Training Loss: 0.1150, Validation Loss: 1.7329\n",
      "Epoch [302/500], Training Loss: 0.1133, Validation Loss: 1.7219\n",
      "Epoch [303/500], Training Loss: 0.1118, Validation Loss: 1.7116\n",
      "Epoch [304/500], Training Loss: 0.1118, Validation Loss: 1.7204\n",
      "Epoch [305/500], Training Loss: 0.1111, Validation Loss: 1.7230\n",
      "Epoch [306/500], Training Loss: 0.1120, Validation Loss: 1.7289\n",
      "Epoch [307/500], Training Loss: 0.1100, Validation Loss: 1.7250\n",
      "Epoch [308/500], Training Loss: 0.1087, Validation Loss: 1.7197\n",
      "Epoch [309/500], Training Loss: 0.1065, Validation Loss: 1.7345\n",
      "Epoch [310/500], Training Loss: 0.1093, Validation Loss: 1.7475\n",
      "Epoch [311/500], Training Loss: 0.1079, Validation Loss: 1.7122\n",
      "Epoch [312/500], Training Loss: 0.1073, Validation Loss: 1.7468\n",
      "Epoch [313/500], Training Loss: 0.1051, Validation Loss: 1.7224\n",
      "Epoch [314/500], Training Loss: 0.1045, Validation Loss: 1.7464\n",
      "Epoch [315/500], Training Loss: 0.1027, Validation Loss: 1.7379\n",
      "Epoch [316/500], Training Loss: 0.1068, Validation Loss: 1.7445\n",
      "Epoch [317/500], Training Loss: 0.1074, Validation Loss: 1.7466\n",
      "Epoch [318/500], Training Loss: 0.1037, Validation Loss: 1.7585\n",
      "Epoch [319/500], Training Loss: 0.1031, Validation Loss: 1.7288\n",
      "Epoch [320/500], Training Loss: 0.0995, Validation Loss: 1.7456\n",
      "Epoch [321/500], Training Loss: 0.0989, Validation Loss: 1.7501\n",
      "Epoch [322/500], Training Loss: 0.0983, Validation Loss: 1.7642\n",
      "Epoch [323/500], Training Loss: 0.0984, Validation Loss: 1.7477\n",
      "Epoch [324/500], Training Loss: 0.0970, Validation Loss: 1.7670\n",
      "Epoch [325/500], Training Loss: 0.0974, Validation Loss: 1.7482\n",
      "Epoch [326/500], Training Loss: 0.0961, Validation Loss: 1.7555\n",
      "Epoch [327/500], Training Loss: 0.0943, Validation Loss: 1.7547\n",
      "Epoch [328/500], Training Loss: 0.0950, Validation Loss: 1.7713\n",
      "Epoch [329/500], Training Loss: 0.0947, Validation Loss: 1.7756\n",
      "Epoch [330/500], Training Loss: 0.0939, Validation Loss: 1.7658\n",
      "Epoch [331/500], Training Loss: 0.0909, Validation Loss: 1.7637\n",
      "Epoch [332/500], Training Loss: 0.0923, Validation Loss: 1.7663\n",
      "Epoch [333/500], Training Loss: 0.0916, Validation Loss: 1.7766\n",
      "Epoch [334/500], Training Loss: 0.0911, Validation Loss: 1.7661\n",
      "Epoch [335/500], Training Loss: 0.0904, Validation Loss: 1.7769\n",
      "Epoch [336/500], Training Loss: 0.0916, Validation Loss: 1.7971\n",
      "Epoch [337/500], Training Loss: 0.0902, Validation Loss: 1.7879\n",
      "Epoch [338/500], Training Loss: 0.0889, Validation Loss: 1.7576\n",
      "Epoch [339/500], Training Loss: 0.0893, Validation Loss: 1.7822\n",
      "Epoch [340/500], Training Loss: 0.0867, Validation Loss: 1.7891\n",
      "Epoch [341/500], Training Loss: 0.0881, Validation Loss: 1.7830\n",
      "Epoch [342/500], Training Loss: 0.0838, Validation Loss: 1.8041\n",
      "Epoch [343/500], Training Loss: 0.0852, Validation Loss: 1.8041\n",
      "Epoch [344/500], Training Loss: 0.0862, Validation Loss: 1.7967\n",
      "Epoch [345/500], Training Loss: 0.0845, Validation Loss: 1.8007\n",
      "Epoch [346/500], Training Loss: 0.0819, Validation Loss: 1.7978\n",
      "Epoch [347/500], Training Loss: 0.0819, Validation Loss: 1.8154\n",
      "Epoch [348/500], Training Loss: 0.0835, Validation Loss: 1.8258\n",
      "Epoch [349/500], Training Loss: 0.0825, Validation Loss: 1.7835\n",
      "Epoch [350/500], Training Loss: 0.0813, Validation Loss: 1.8084\n",
      "Epoch [351/500], Training Loss: 0.0802, Validation Loss: 1.7960\n",
      "Epoch [352/500], Training Loss: 0.0800, Validation Loss: 1.8131\n",
      "Epoch [353/500], Training Loss: 0.0815, Validation Loss: 1.8109\n",
      "Epoch [354/500], Training Loss: 0.0784, Validation Loss: 1.8144\n",
      "Epoch [355/500], Training Loss: 0.0763, Validation Loss: 1.8041\n",
      "Epoch [356/500], Training Loss: 0.0764, Validation Loss: 1.8072\n",
      "Epoch [357/500], Training Loss: 0.0761, Validation Loss: 1.8303\n",
      "Epoch [358/500], Training Loss: 0.0753, Validation Loss: 1.8317\n",
      "Epoch [359/500], Training Loss: 0.0741, Validation Loss: 1.8199\n",
      "Epoch [360/500], Training Loss: 0.0744, Validation Loss: 1.8243\n",
      "Epoch [361/500], Training Loss: 0.0770, Validation Loss: 1.8219\n",
      "Epoch [362/500], Training Loss: 0.0744, Validation Loss: 1.8128\n",
      "Epoch [363/500], Training Loss: 0.0736, Validation Loss: 1.8240\n",
      "Epoch [364/500], Training Loss: 0.0729, Validation Loss: 1.8478\n",
      "Epoch [365/500], Training Loss: 0.0735, Validation Loss: 1.8338\n",
      "Epoch [366/500], Training Loss: 0.0714, Validation Loss: 1.8295\n",
      "Epoch [367/500], Training Loss: 0.0721, Validation Loss: 1.8360\n",
      "Epoch [368/500], Training Loss: 0.0700, Validation Loss: 1.8377\n",
      "Epoch [369/500], Training Loss: 0.0704, Validation Loss: 1.8563\n",
      "Epoch [370/500], Training Loss: 0.0686, Validation Loss: 1.8453\n",
      "Epoch [371/500], Training Loss: 0.0690, Validation Loss: 1.8432\n",
      "Epoch [372/500], Training Loss: 0.0688, Validation Loss: 1.8524\n",
      "Epoch [373/500], Training Loss: 0.0674, Validation Loss: 1.8538\n",
      "Epoch [374/500], Training Loss: 0.0683, Validation Loss: 1.8648\n",
      "Epoch [375/500], Training Loss: 0.0675, Validation Loss: 1.8467\n",
      "Epoch [376/500], Training Loss: 0.0666, Validation Loss: 1.8473\n",
      "Epoch [377/500], Training Loss: 0.0654, Validation Loss: 1.8566\n",
      "Epoch [378/500], Training Loss: 0.0644, Validation Loss: 1.8523\n",
      "Epoch [379/500], Training Loss: 0.0647, Validation Loss: 1.8592\n",
      "Epoch [380/500], Training Loss: 0.0653, Validation Loss: 1.8942\n",
      "Epoch [381/500], Training Loss: 0.0673, Validation Loss: 1.8528\n",
      "Epoch [382/500], Training Loss: 0.0619, Validation Loss: 1.8672\n",
      "Epoch [383/500], Training Loss: 0.0635, Validation Loss: 1.8719\n",
      "Epoch [384/500], Training Loss: 0.0622, Validation Loss: 1.8741\n",
      "Epoch [385/500], Training Loss: 0.0617, Validation Loss: 1.8542\n",
      "Epoch [386/500], Training Loss: 0.0638, Validation Loss: 1.8932\n",
      "Epoch [387/500], Training Loss: 0.0620, Validation Loss: 1.8694\n",
      "Epoch [388/500], Training Loss: 0.0626, Validation Loss: 1.8833\n",
      "Epoch [389/500], Training Loss: 0.0598, Validation Loss: 1.8882\n",
      "Epoch [390/500], Training Loss: 0.0591, Validation Loss: 1.8991\n",
      "Epoch [391/500], Training Loss: 0.0586, Validation Loss: 1.8881\n",
      "Epoch [392/500], Training Loss: 0.0585, Validation Loss: 1.9045\n",
      "Epoch [393/500], Training Loss: 0.0598, Validation Loss: 1.8984\n",
      "Epoch [394/500], Training Loss: 0.0577, Validation Loss: 1.9075\n",
      "Epoch [395/500], Training Loss: 0.0572, Validation Loss: 1.8875\n",
      "Epoch [396/500], Training Loss: 0.0574, Validation Loss: 1.9020\n",
      "Epoch [397/500], Training Loss: 0.0570, Validation Loss: 1.9009\n",
      "Epoch [398/500], Training Loss: 0.0556, Validation Loss: 1.9103\n",
      "Epoch [399/500], Training Loss: 0.0555, Validation Loss: 1.9143\n",
      "Epoch [400/500], Training Loss: 0.0556, Validation Loss: 1.9168\n",
      "Epoch [401/500], Training Loss: 0.0584, Validation Loss: 1.8980\n",
      "Epoch [402/500], Training Loss: 0.0563, Validation Loss: 1.8991\n",
      "Epoch [403/500], Training Loss: 0.0541, Validation Loss: 1.9162\n",
      "Epoch [404/500], Training Loss: 0.0533, Validation Loss: 1.9133\n",
      "Epoch [405/500], Training Loss: 0.0540, Validation Loss: 1.9224\n",
      "Epoch [406/500], Training Loss: 0.0529, Validation Loss: 1.9408\n",
      "Epoch [407/500], Training Loss: 0.0535, Validation Loss: 1.9380\n",
      "Epoch [408/500], Training Loss: 0.0520, Validation Loss: 1.9354\n",
      "Epoch [409/500], Training Loss: 0.0520, Validation Loss: 1.9403\n",
      "Epoch [410/500], Training Loss: 0.0531, Validation Loss: 1.9254\n",
      "Epoch [411/500], Training Loss: 0.0516, Validation Loss: 1.9290\n",
      "Epoch [412/500], Training Loss: 0.0513, Validation Loss: 1.9288\n",
      "Epoch [413/500], Training Loss: 0.0497, Validation Loss: 1.9520\n",
      "Epoch [414/500], Training Loss: 0.0516, Validation Loss: 1.9428\n",
      "Epoch [415/500], Training Loss: 0.0503, Validation Loss: 1.9505\n",
      "Epoch [416/500], Training Loss: 0.0502, Validation Loss: 1.9357\n",
      "Epoch [417/500], Training Loss: 0.0495, Validation Loss: 1.9464\n",
      "Epoch [418/500], Training Loss: 0.0481, Validation Loss: 1.9510\n",
      "Epoch [419/500], Training Loss: 0.0480, Validation Loss: 1.9476\n",
      "Epoch [420/500], Training Loss: 0.0470, Validation Loss: 1.9595\n",
      "Epoch [421/500], Training Loss: 0.0495, Validation Loss: 1.9718\n",
      "Epoch [422/500], Training Loss: 0.0476, Validation Loss: 1.9525\n",
      "Epoch [423/500], Training Loss: 0.0467, Validation Loss: 1.9659\n",
      "Epoch [424/500], Training Loss: 0.0472, Validation Loss: 1.9746\n",
      "Epoch [425/500], Training Loss: 0.0464, Validation Loss: 1.9616\n",
      "Epoch [426/500], Training Loss: 0.0448, Validation Loss: 1.9556\n",
      "Epoch [427/500], Training Loss: 0.0460, Validation Loss: 1.9778\n",
      "Epoch [428/500], Training Loss: 0.0479, Validation Loss: 1.9875\n",
      "Epoch [429/500], Training Loss: 0.0460, Validation Loss: 1.9527\n",
      "Epoch [430/500], Training Loss: 0.0447, Validation Loss: 1.9660\n",
      "Epoch [431/500], Training Loss: 0.0444, Validation Loss: 1.9665\n",
      "Epoch [432/500], Training Loss: 0.0451, Validation Loss: 1.9738\n",
      "Epoch [433/500], Training Loss: 0.0439, Validation Loss: 1.9884\n",
      "Epoch [434/500], Training Loss: 0.0434, Validation Loss: 1.9813\n",
      "Epoch [435/500], Training Loss: 0.0445, Validation Loss: 1.9862\n",
      "Epoch [436/500], Training Loss: 0.0446, Validation Loss: 1.9677\n",
      "Epoch [437/500], Training Loss: 0.0419, Validation Loss: 1.9555\n",
      "Epoch [438/500], Training Loss: 0.0424, Validation Loss: 1.9793\n",
      "Epoch [439/500], Training Loss: 0.0413, Validation Loss: 1.9795\n",
      "Epoch [440/500], Training Loss: 0.0407, Validation Loss: 2.0053\n",
      "Epoch [441/500], Training Loss: 0.0405, Validation Loss: 1.9944\n",
      "Epoch [442/500], Training Loss: 0.0395, Validation Loss: 1.9967\n",
      "Epoch [443/500], Training Loss: 0.0411, Validation Loss: 2.0064\n",
      "Epoch [444/500], Training Loss: 0.0402, Validation Loss: 2.0119\n",
      "Epoch [445/500], Training Loss: 0.0420, Validation Loss: 1.9877\n",
      "Epoch [446/500], Training Loss: 0.0420, Validation Loss: 1.9976\n",
      "Epoch [447/500], Training Loss: 0.0409, Validation Loss: 1.9997\n",
      "Epoch [448/500], Training Loss: 0.0408, Validation Loss: 2.0055\n",
      "Epoch [449/500], Training Loss: 0.0391, Validation Loss: 1.9801\n",
      "Epoch [450/500], Training Loss: 0.0384, Validation Loss: 2.0146\n",
      "Epoch [451/500], Training Loss: 0.0380, Validation Loss: 2.0347\n",
      "Epoch [452/500], Training Loss: 0.0402, Validation Loss: 2.0203\n",
      "Epoch [453/500], Training Loss: 0.0385, Validation Loss: 2.0214\n",
      "Epoch [454/500], Training Loss: 0.0369, Validation Loss: 2.0236\n",
      "Epoch [455/500], Training Loss: 0.0375, Validation Loss: 2.0011\n",
      "Epoch [456/500], Training Loss: 0.0374, Validation Loss: 2.0485\n",
      "Epoch [457/500], Training Loss: 0.0365, Validation Loss: 2.0175\n",
      "Epoch [458/500], Training Loss: 0.0356, Validation Loss: 2.0184\n",
      "Epoch [459/500], Training Loss: 0.0370, Validation Loss: 2.0488\n",
      "Epoch [460/500], Training Loss: 0.0375, Validation Loss: 2.0304\n",
      "Epoch [461/500], Training Loss: 0.0359, Validation Loss: 2.0398\n",
      "Epoch [462/500], Training Loss: 0.0357, Validation Loss: 2.0368\n",
      "Epoch [463/500], Training Loss: 0.0363, Validation Loss: 2.0383\n",
      "Epoch [464/500], Training Loss: 0.0350, Validation Loss: 2.0388\n",
      "Epoch [465/500], Training Loss: 0.0353, Validation Loss: 2.0446\n",
      "Epoch [466/500], Training Loss: 0.0344, Validation Loss: 2.0353\n",
      "Epoch [467/500], Training Loss: 0.0359, Validation Loss: 2.0266\n",
      "Epoch [468/500], Training Loss: 0.0338, Validation Loss: 2.0550\n",
      "Epoch [469/500], Training Loss: 0.0329, Validation Loss: 2.0516\n",
      "Epoch [470/500], Training Loss: 0.0328, Validation Loss: 2.0638\n",
      "Epoch [471/500], Training Loss: 0.0344, Validation Loss: 2.0335\n",
      "Epoch [472/500], Training Loss: 0.0331, Validation Loss: 2.0552\n",
      "Epoch [473/500], Training Loss: 0.0317, Validation Loss: 2.0465\n",
      "Epoch [474/500], Training Loss: 0.0325, Validation Loss: 2.0619\n",
      "Epoch [475/500], Training Loss: 0.0330, Validation Loss: 2.0675\n",
      "Epoch [476/500], Training Loss: 0.0318, Validation Loss: 2.0735\n",
      "Epoch [477/500], Training Loss: 0.0317, Validation Loss: 2.0542\n",
      "Epoch [478/500], Training Loss: 0.0320, Validation Loss: 2.0550\n",
      "Epoch [479/500], Training Loss: 0.0316, Validation Loss: 2.0678\n",
      "Epoch [480/500], Training Loss: 0.0313, Validation Loss: 2.0618\n",
      "Epoch [481/500], Training Loss: 0.0324, Validation Loss: 2.0721\n",
      "Epoch [482/500], Training Loss: 0.0324, Validation Loss: 2.0628\n",
      "Epoch [483/500], Training Loss: 0.0314, Validation Loss: 2.0589\n",
      "Epoch [484/500], Training Loss: 0.0294, Validation Loss: 2.0989\n",
      "Epoch [485/500], Training Loss: 0.0303, Validation Loss: 2.0952\n",
      "Epoch [486/500], Training Loss: 0.0304, Validation Loss: 2.0934\n",
      "Epoch [487/500], Training Loss: 0.0302, Validation Loss: 2.0806\n",
      "Epoch [488/500], Training Loss: 0.0308, Validation Loss: 2.0855\n",
      "Epoch [489/500], Training Loss: 0.0288, Validation Loss: 2.0806\n",
      "Epoch [490/500], Training Loss: 0.0283, Validation Loss: 2.0980\n",
      "Epoch [491/500], Training Loss: 0.0289, Validation Loss: 2.0875\n",
      "Epoch [492/500], Training Loss: 0.0281, Validation Loss: 2.1108\n",
      "Epoch [493/500], Training Loss: 0.0278, Validation Loss: 2.0820\n",
      "Epoch [494/500], Training Loss: 0.0280, Validation Loss: 2.1031\n",
      "Epoch [495/500], Training Loss: 0.0286, Validation Loss: 2.0985\n",
      "Epoch [496/500], Training Loss: 0.0279, Validation Loss: 2.0929\n",
      "Epoch [497/500], Training Loss: 0.0265, Validation Loss: 2.1004\n",
      "Epoch [498/500], Training Loss: 0.0261, Validation Loss: 2.0905\n",
      "Epoch [499/500], Training Loss: 0.0272, Validation Loss: 2.1162\n",
      "Epoch [500/500], Training Loss: 0.0259, Validation Loss: 2.1169\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Train model SimpleANN\n",
    "complexANN = ComplexANN(input_size, output_size, hidden_sizes=[64, 64])\n",
    "# Add weight decay\n",
    "optimizer = torch.optim.Adam(complexANN.parameters(), lr=3e-4, weight_decay=1e-6)\n",
    "train(complexANN, criterion, optimizer, num_epochs, train_loader, valid_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Testing\n",
    "\n",
    "Metrics:\n",
    "- Accuracy\n",
    "- Precision and Recall\n",
    "- F1 Score\n",
    "- Confusion Matrix\n",
    "- ROC Curve and AUC\n",
    "\n",
    "Outcomes:\n",
    "- If the model is underperforming, consider increasing the number of **layers** or **units**.\n",
    "- If the model is overfitting, try reducing the complexity, adding **dropout** layers, or increasing **regularization**.\n",
    "\n",
    "\n",
    "So in general, I will create a **SimpleANN**, a **ComplexANN** and a **FinalANN** and compare them performance wise.\n",
    "The number of **epochs** and **learning rate** will be the same when comparing their performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlshit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
